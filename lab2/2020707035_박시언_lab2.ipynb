{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Lab2",
   "metadata": {
    "collapsed": false
   },
   "id": "a6bd141f0d5af7d1"
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import urllib.request\n",
    "import urllib.request\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print('-------------------')\n",
    "print('|     lab2        |')\n",
    "print('-------------------')\n",
    "\n",
    "if os.path.exists('/semi.csv'):\n",
    "    '''\n",
    "    혹시 파일이 생기지 않는다면, 아래 두 줄의 스크립스틑 파이썬 .py파일로 만들어서 실행하면 됩니다. \n",
    "    '''\n",
    "    url = \"https://drive.google.com/uc?export=download&id=1XCU0eo2xZ03xhxJhdrCnVjduCoaBQ7kJ\"\n",
    "    urllib.request.urlretrieve(url, \"semi.csv\")  # save in a file\n",
    "else:\n",
    "    print('data already exist')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T17:05:37.367115Z",
     "start_time": "2024-09-29T17:05:37.354602Z"
    }
   },
   "id": "f2e2212a013d25ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "|     lab2        |\n",
      "-------------------\n",
      "data already exist\n"
     ]
    }
   ],
   "execution_count": 912
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('semi.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T17:05:37.526934Z",
     "start_time": "2024-09-29T17:05:37.455955Z"
    }
   },
   "id": "190d26f2c203f742",
   "outputs": [],
   "execution_count": 913
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 데이터 null값을 전체 컬럼에서 구합니다. 41951개의 Null data가 존재합니다.",
   "id": "ae35253ac4190895"
  },
  {
   "cell_type": "code",
   "source": "df.isnull().sum().sum()\n",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T17:05:37.532847Z",
     "start_time": "2024-09-29T17:05:37.528274Z"
    }
   },
   "id": "84861c09a084a62f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(41951)"
      ]
     },
     "execution_count": 914,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 914
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 데이터 info를 확인해보니 Object로 문자열로 인코딩된 데이터가 한 개 존재합니다.",
   "id": "f73fa16f6b315152"
  },
  {
   "cell_type": "code",
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T17:05:37.543922Z",
     "start_time": "2024-09-29T17:05:37.533541Z"
    }
   },
   "id": "465ab6554b516868",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1567 entries, 0 to 1566\n",
      "Columns: 592 entries, Time to Pass/Fail\n",
      "dtypes: float64(590), int64(1), object(1)\n",
      "memory usage: 7.1+ MB\n"
     ]
    }
   ],
   "execution_count": 915
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T17:05:37.550089Z",
     "start_time": "2024-09-29T17:05:37.545530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_collinear_features(x, threshold = 0.5):\n",
    "    corr_matrix = x.corr()\n",
    "    iters = range(len(corr_matrix.columns) - 1)\n",
    "    drop_cols = []\n",
    "    \n",
    "    for i in iters:\n",
    "        for j in range(i+1):\n",
    "            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n",
    "            col = item.columns\n",
    "            row = item.index\n",
    "            # 절대값을 씌우는 이유는\n",
    "            # corr 절대값이 높은거를 제거하면 되기 때문에\n",
    "            val = abs(item.values)\n",
    "            \n",
    "            if val >= threshold:\n",
    "                print(col.values[0], '|', row.values[0], '|', round(val[0][0], 2))\n",
    "                drop_cols.append(col.values[0])\n",
    "                \n",
    "    drops = set(drop_cols)\n",
    "    x = x.drop(columns=drops)\n",
    "            \n",
    "    return x\n"
   ],
   "id": "dc3ed48f8c1de6fc",
   "outputs": [],
   "execution_count": 916
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 데이터 decribe를 확인해보니 std가 상당히 큰 걸 알 수 있습니다. \n",
    "### 또한 0 or 100으로만 이루어진 std =0 인 데이터가 존재합니다."
   ],
   "id": "e4274d25dc9ef2eb"
  },
  {
   "cell_type": "code",
   "source": [
    "df.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T17:05:37.804248Z",
     "start_time": "2024-09-29T17:05:37.550923Z"
    }
   },
   "id": "278d8e87cafdf2dc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                 0            1            2            3            4  \\\n",
       "count  1561.000000  1560.000000  1553.000000  1553.000000  1553.000000   \n",
       "mean   3014.452896  2495.850231  2200.547318  1396.376627     4.197013   \n",
       "std      73.621787    80.407705    29.513152   441.691640    56.355540   \n",
       "min    2743.240000  2158.750000  2060.660000     0.000000     0.681500   \n",
       "25%    2966.260000  2452.247500  2181.044400  1081.875800     1.017700   \n",
       "50%    3011.490000  2499.405000  2201.066700  1285.214400     1.316800   \n",
       "75%    3056.650000  2538.822500  2218.055500  1591.223500     1.525700   \n",
       "max    3356.350000  2846.440000  2315.266700  3715.041700  1114.536600   \n",
       "\n",
       "            5            6            7            8            9  ...  \\\n",
       "count  1553.0  1553.000000  1558.000000  1565.000000  1565.000000  ...   \n",
       "mean    100.0   101.112908     0.121822     1.462862    -0.000841  ...   \n",
       "std       0.0     6.237214     0.008961     0.073897     0.015116  ...   \n",
       "min     100.0    82.131100     0.000000     1.191000    -0.053400  ...   \n",
       "25%     100.0    97.920000     0.121100     1.411200    -0.010800  ...   \n",
       "50%     100.0   101.512200     0.122400     1.461600    -0.001300  ...   \n",
       "75%     100.0   104.586700     0.123800     1.516900     0.008400  ...   \n",
       "max     100.0   129.252200     0.128600     1.656400     0.074900  ...   \n",
       "\n",
       "              581          582          583          584          585  \\\n",
       "count  618.000000  1566.000000  1566.000000  1566.000000  1566.000000   \n",
       "mean    97.934373     0.500096     0.015318     0.003847     3.067826   \n",
       "std     87.520966     0.003404     0.017180     0.003720     3.578033   \n",
       "min      0.000000     0.477800     0.006000     0.001700     1.197500   \n",
       "25%     46.184900     0.497900     0.011600     0.003100     2.306500   \n",
       "50%     72.288900     0.500200     0.013800     0.003600     2.757650   \n",
       "75%    116.539150     0.502375     0.016500     0.004100     3.295175   \n",
       "max    737.304800     0.509800     0.476600     0.104500    99.303200   \n",
       "\n",
       "               586          587          588          589    Pass/Fail  \n",
       "count  1566.000000  1566.000000  1566.000000  1566.000000  1567.000000  \n",
       "mean      0.021458     0.016475     0.005283    99.670066    -0.867262  \n",
       "std       0.012358     0.008808     0.002867    93.891919     0.498010  \n",
       "min      -0.016900     0.003200     0.001000     0.000000    -1.000000  \n",
       "25%       0.013425     0.010600     0.003300    44.368600    -1.000000  \n",
       "50%       0.020500     0.014800     0.004600    71.900500    -1.000000  \n",
       "75%       0.027600     0.020300     0.006400   114.749700    -1.000000  \n",
       "max       0.102800     0.079900     0.028600   737.304800     1.000000  \n",
       "\n",
       "[8 rows x 591 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>581</th>\n",
       "      <th>582</th>\n",
       "      <th>583</th>\n",
       "      <th>584</th>\n",
       "      <th>585</th>\n",
       "      <th>586</th>\n",
       "      <th>587</th>\n",
       "      <th>588</th>\n",
       "      <th>589</th>\n",
       "      <th>Pass/Fail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1561.000000</td>\n",
       "      <td>1560.000000</td>\n",
       "      <td>1553.000000</td>\n",
       "      <td>1553.000000</td>\n",
       "      <td>1553.000000</td>\n",
       "      <td>1553.0</td>\n",
       "      <td>1553.000000</td>\n",
       "      <td>1558.000000</td>\n",
       "      <td>1565.000000</td>\n",
       "      <td>1565.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>618.000000</td>\n",
       "      <td>1566.000000</td>\n",
       "      <td>1566.000000</td>\n",
       "      <td>1566.000000</td>\n",
       "      <td>1566.000000</td>\n",
       "      <td>1566.000000</td>\n",
       "      <td>1566.000000</td>\n",
       "      <td>1566.000000</td>\n",
       "      <td>1566.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3014.452896</td>\n",
       "      <td>2495.850231</td>\n",
       "      <td>2200.547318</td>\n",
       "      <td>1396.376627</td>\n",
       "      <td>4.197013</td>\n",
       "      <td>100.0</td>\n",
       "      <td>101.112908</td>\n",
       "      <td>0.121822</td>\n",
       "      <td>1.462862</td>\n",
       "      <td>-0.000841</td>\n",
       "      <td>...</td>\n",
       "      <td>97.934373</td>\n",
       "      <td>0.500096</td>\n",
       "      <td>0.015318</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>3.067826</td>\n",
       "      <td>0.021458</td>\n",
       "      <td>0.016475</td>\n",
       "      <td>0.005283</td>\n",
       "      <td>99.670066</td>\n",
       "      <td>-0.867262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>73.621787</td>\n",
       "      <td>80.407705</td>\n",
       "      <td>29.513152</td>\n",
       "      <td>441.691640</td>\n",
       "      <td>56.355540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.237214</td>\n",
       "      <td>0.008961</td>\n",
       "      <td>0.073897</td>\n",
       "      <td>0.015116</td>\n",
       "      <td>...</td>\n",
       "      <td>87.520966</td>\n",
       "      <td>0.003404</td>\n",
       "      <td>0.017180</td>\n",
       "      <td>0.003720</td>\n",
       "      <td>3.578033</td>\n",
       "      <td>0.012358</td>\n",
       "      <td>0.008808</td>\n",
       "      <td>0.002867</td>\n",
       "      <td>93.891919</td>\n",
       "      <td>0.498010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2743.240000</td>\n",
       "      <td>2158.750000</td>\n",
       "      <td>2060.660000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.681500</td>\n",
       "      <td>100.0</td>\n",
       "      <td>82.131100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.191000</td>\n",
       "      <td>-0.053400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.477800</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>1.197500</td>\n",
       "      <td>-0.016900</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2966.260000</td>\n",
       "      <td>2452.247500</td>\n",
       "      <td>2181.044400</td>\n",
       "      <td>1081.875800</td>\n",
       "      <td>1.017700</td>\n",
       "      <td>100.0</td>\n",
       "      <td>97.920000</td>\n",
       "      <td>0.121100</td>\n",
       "      <td>1.411200</td>\n",
       "      <td>-0.010800</td>\n",
       "      <td>...</td>\n",
       "      <td>46.184900</td>\n",
       "      <td>0.497900</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>2.306500</td>\n",
       "      <td>0.013425</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>44.368600</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3011.490000</td>\n",
       "      <td>2499.405000</td>\n",
       "      <td>2201.066700</td>\n",
       "      <td>1285.214400</td>\n",
       "      <td>1.316800</td>\n",
       "      <td>100.0</td>\n",
       "      <td>101.512200</td>\n",
       "      <td>0.122400</td>\n",
       "      <td>1.461600</td>\n",
       "      <td>-0.001300</td>\n",
       "      <td>...</td>\n",
       "      <td>72.288900</td>\n",
       "      <td>0.500200</td>\n",
       "      <td>0.013800</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>2.757650</td>\n",
       "      <td>0.020500</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>71.900500</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3056.650000</td>\n",
       "      <td>2538.822500</td>\n",
       "      <td>2218.055500</td>\n",
       "      <td>1591.223500</td>\n",
       "      <td>1.525700</td>\n",
       "      <td>100.0</td>\n",
       "      <td>104.586700</td>\n",
       "      <td>0.123800</td>\n",
       "      <td>1.516900</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>...</td>\n",
       "      <td>116.539150</td>\n",
       "      <td>0.502375</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>3.295175</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>114.749700</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3356.350000</td>\n",
       "      <td>2846.440000</td>\n",
       "      <td>2315.266700</td>\n",
       "      <td>3715.041700</td>\n",
       "      <td>1114.536600</td>\n",
       "      <td>100.0</td>\n",
       "      <td>129.252200</td>\n",
       "      <td>0.128600</td>\n",
       "      <td>1.656400</td>\n",
       "      <td>0.074900</td>\n",
       "      <td>...</td>\n",
       "      <td>737.304800</td>\n",
       "      <td>0.509800</td>\n",
       "      <td>0.476600</td>\n",
       "      <td>0.104500</td>\n",
       "      <td>99.303200</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>0.079900</td>\n",
       "      <td>0.028600</td>\n",
       "      <td>737.304800</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 591 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 917,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 917
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### NaN 즉, 발산해버린 센서 데이터도 존재하는 것을 볼 수 있습니다.",
   "id": "18e51091e7e72c9a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T17:05:37.808384Z",
     "start_time": "2024-09-29T17:05:37.804943Z"
    }
   },
   "cell_type": "code",
   "source": "df.isna().sum().sum()",
   "id": "ea5972addd8cbee6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(41951)"
      ]
     },
     "execution_count": 918,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 918
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## DuplicatedHandler\n",
    "\n",
    "### 1. 데이터의 중복이 존재하는 것을 볼 수 있습니다.\n",
    "### 2. 모든 컬럼을 돌면서 st, me, mx, mn : tuple 값을 stack에 넣고 iter합니다.\n",
    "### 3. stack에 동일한 분포를 갖는 컬럼이 있으면 idx를 저장하고 drop at axis = 1을 통해 drop합니다."
   ],
   "id": "e3c6531054296364"
  },
  {
   "cell_type": "code",
   "source": [
    "def DuplicatedHandler(df: pd.DataFrame):\n",
    "    features = range(590)\n",
    "    stack = []\n",
    "    idx = []\n",
    "    for i, fe in enumerate(features):\n",
    "        st = df[str(fe)].std()\n",
    "        me = df[str(fe)].mean()\n",
    "        mx = df[str(fe)].max()\n",
    "        mn = df[str(fe)].min()\n",
    "        if (st, me, mx, mn) in stack:\n",
    "            idx.append(fe)\n",
    "            print('duplicated!', i)\n",
    "        else:\n",
    "            stack.append((st, me, mx, mn))\n",
    "\n",
    "    for i in idx:\n",
    "        df = df.drop(str(i), axis=1)\n",
    "    df = df.drop(['Time'], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "df = DuplicatedHandler(df)\n",
    "df = remove_collinear_features(df)\n",
    "feature = df.drop('Pass/Fail', axis=1)\n",
    "target = df['Pass/Fail']\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature, target, \\\n",
    "                                                    test_size=0.2, random_state=11, stratify=target)\n",
    "\n",
    "X_train, X_test, y_train, y_test = pd.DataFrame(X_train), pd.DataFrame(X_test), y_train, y_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T17:05:40.867738Z",
     "start_time": "2024-09-29T17:05:37.808940Z"
    }
   },
   "id": "ac9b3a7cc56d3d45",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicated! 52\n",
      "duplicated! 69\n",
      "duplicated! 97\n",
      "duplicated! 141\n",
      "duplicated! 149\n",
      "duplicated! 178\n",
      "duplicated! 179\n",
      "duplicated! 186\n",
      "duplicated! 189\n",
      "duplicated! 190\n",
      "duplicated! 191\n",
      "duplicated! 192\n",
      "duplicated! 193\n",
      "duplicated! 194\n",
      "duplicated! 226\n",
      "duplicated! 229\n",
      "duplicated! 230\n",
      "duplicated! 231\n",
      "duplicated! 232\n",
      "duplicated! 233\n",
      "duplicated! 234\n",
      "duplicated! 235\n",
      "duplicated! 236\n",
      "duplicated! 237\n",
      "duplicated! 240\n",
      "duplicated! 241\n",
      "duplicated! 242\n",
      "duplicated! 243\n",
      "duplicated! 256\n",
      "duplicated! 257\n",
      "duplicated! 258\n",
      "duplicated! 259\n",
      "duplicated! 260\n",
      "duplicated! 261\n",
      "duplicated! 262\n",
      "duplicated! 263\n",
      "duplicated! 264\n",
      "duplicated! 265\n",
      "duplicated! 266\n",
      "duplicated! 276\n",
      "duplicated! 284\n",
      "duplicated! 313\n",
      "duplicated! 314\n",
      "duplicated! 315\n",
      "duplicated! 322\n",
      "duplicated! 325\n",
      "duplicated! 326\n",
      "duplicated! 327\n",
      "duplicated! 328\n",
      "duplicated! 329\n",
      "duplicated! 330\n",
      "duplicated! 364\n",
      "duplicated! 369\n",
      "duplicated! 370\n",
      "duplicated! 371\n",
      "duplicated! 372\n",
      "duplicated! 373\n",
      "duplicated! 374\n",
      "duplicated! 375\n",
      "duplicated! 378\n",
      "duplicated! 379\n",
      "duplicated! 380\n",
      "duplicated! 381\n",
      "duplicated! 394\n",
      "duplicated! 395\n",
      "duplicated! 396\n",
      "duplicated! 397\n",
      "duplicated! 398\n",
      "duplicated! 399\n",
      "duplicated! 400\n",
      "duplicated! 401\n",
      "duplicated! 402\n",
      "duplicated! 403\n",
      "duplicated! 404\n",
      "duplicated! 414\n",
      "duplicated! 422\n",
      "duplicated! 449\n",
      "duplicated! 450\n",
      "duplicated! 451\n",
      "duplicated! 458\n",
      "duplicated! 461\n",
      "duplicated! 462\n",
      "duplicated! 463\n",
      "duplicated! 464\n",
      "duplicated! 465\n",
      "duplicated! 466\n",
      "duplicated! 481\n",
      "duplicated! 498\n",
      "duplicated! 501\n",
      "duplicated! 502\n",
      "duplicated! 503\n",
      "duplicated! 504\n",
      "duplicated! 505\n",
      "duplicated! 506\n",
      "duplicated! 507\n",
      "duplicated! 508\n",
      "duplicated! 509\n",
      "duplicated! 512\n",
      "duplicated! 513\n",
      "duplicated! 514\n",
      "duplicated! 515\n",
      "duplicated! 528\n",
      "duplicated! 529\n",
      "duplicated! 530\n",
      "duplicated! 531\n",
      "duplicated! 532\n",
      "duplicated! 533\n",
      "duplicated! 534\n",
      "duplicated! 535\n",
      "duplicated! 536\n",
      "duplicated! 537\n",
      "duplicated! 538\n",
      "6 | 3 | 0.69\n",
      "7 | 4 | 0.92\n",
      "16 | 12 | 0.55\n",
      "17 | 11 | 0.79\n",
      "18 | 12 | 0.69\n",
      "22 | 21 | 0.73\n",
      "26 | 22 | 0.53\n",
      "26 | 25 | 0.82\n",
      "27 | 25 | 0.98\n",
      "27 | 26 | 0.79\n",
      "29 | 28 | 0.52\n",
      "30 | 28 | 0.58\n",
      "30 | 29 | 0.86\n",
      "31 | 21 | 0.6\n",
      "31 | 25 | 0.62\n",
      "31 | 27 | 0.65\n",
      "33 | 32 | 0.67\n",
      "34 | 32 | 0.75\n",
      "35 | 32 | 0.51\n",
      "35 | 34 | 0.77\n",
      "36 | 32 | 0.75\n",
      "36 | 34 | 1.0\n",
      "36 | 35 | 0.77\n",
      "39 | 32 | 0.53\n",
      "39 | 34 | 0.8\n",
      "39 | 35 | 0.58\n",
      "39 | 36 | 0.8\n",
      "46 | 45 | 0.81\n",
      "48 | 44 | 0.59\n",
      "48 | 45 | 0.53\n",
      "48 | 47 | 0.64\n",
      "50 | 43 | 0.64\n",
      "50 | 45 | 0.56\n",
      "50 | 46 | 0.9\n",
      "51 | 47 | 0.71\n",
      "54 | 53 | 0.94\n",
      "55 | 43 | 0.59\n",
      "56 | 43 | 0.59\n",
      "56 | 50 | 0.54\n",
      "58 | 56 | 0.52\n",
      "60 | 43 | 0.9\n",
      "60 | 46 | 0.61\n",
      "60 | 50 | 0.7\n",
      "60 | 55 | 0.55\n",
      "60 | 56 | 0.59\n",
      "61 | 44 | 0.53\n",
      "61 | 45 | 0.67\n",
      "62 | 45 | 0.64\n",
      "62 | 46 | 0.53\n",
      "62 | 61 | 0.61\n",
      "65 | 63 | 0.5\n",
      "65 | 64 | 0.84\n",
      "66 | 45 | 0.61\n",
      "66 | 46 | 0.82\n",
      "66 | 50 | 0.76\n",
      "66 | 60 | 0.62\n",
      "66 | 61 | 0.53\n",
      "66 | 62 | 0.67\n",
      "67 | 62 | 0.52\n",
      "67 | 63 | 0.6\n",
      "70 | 45 | 0.59\n",
      "70 | 46 | 0.8\n",
      "70 | 50 | 0.77\n",
      "70 | 60 | 0.56\n",
      "70 | 61 | 0.5\n",
      "70 | 62 | 0.73\n",
      "70 | 66 | 0.9\n",
      "73 | 72 | 0.84\n",
      "96 | 94 | 0.96\n",
      "98 | 94 | 0.84\n",
      "98 | 96 | 0.87\n",
      "101 | 94 | 0.73\n",
      "101 | 96 | 0.79\n",
      "101 | 98 | 0.91\n",
      "102 | 91 | 0.56\n",
      "104 | 99 | 0.99\n",
      "105 | 92 | 0.99\n",
      "106 | 93 | 0.99\n",
      "109 | 72 | 0.51\n",
      "115 | 90 | 0.7\n",
      "123 | 121 | 0.94\n",
      "124 | 121 | 0.89\n",
      "124 | 123 | 0.86\n",
      "125 | 122 | 0.71\n",
      "127 | 122 | 0.96\n",
      "127 | 125 | 0.63\n",
      "128 | 126 | 0.51\n",
      "130 | 122 | 0.83\n",
      "130 | 125 | 0.62\n",
      "130 | 127 | 0.79\n",
      "133 | 121 | 0.7\n",
      "133 | 123 | 0.64\n",
      "133 | 124 | 0.63\n",
      "137 | 136 | 0.67\n",
      "140 | 4 | 1.0\n",
      "140 | 7 | 0.92\n",
      "142 | 4 | 0.67\n",
      "142 | 7 | 0.56\n",
      "142 | 140 | 0.67\n",
      "146 | 145 | 0.55\n",
      "147 | 12 | 0.54\n",
      "147 | 16 | 0.89\n",
      "148 | 12 | 0.56\n",
      "148 | 16 | 0.97\n",
      "148 | 147 | 0.89\n",
      "150 | 14 | 0.63\n",
      "152 | 12 | 0.56\n",
      "152 | 16 | 0.98\n",
      "152 | 147 | 0.9\n",
      "152 | 148 | 0.99\n",
      "154 | 12 | 0.51\n",
      "154 | 16 | 0.87\n",
      "154 | 147 | 0.8\n",
      "154 | 148 | 0.94\n",
      "154 | 152 | 0.89\n",
      "155 | 19 | 0.81\n",
      "157 | 85 | 0.94\n",
      "158 | 85 | 0.87\n",
      "159 | 21 | 0.54\n",
      "159 | 26 | 0.56\n",
      "160 | 159 | 0.55\n",
      "163 | 21 | 0.6\n",
      "163 | 22 | 0.55\n",
      "163 | 25 | 0.5\n",
      "163 | 26 | 0.71\n",
      "163 | 159 | 0.76\n",
      "163 | 160 | 0.63\n",
      "164 | 21 | 0.61\n",
      "164 | 22 | 0.59\n",
      "164 | 25 | 0.57\n",
      "164 | 26 | 0.77\n",
      "164 | 27 | 0.56\n",
      "164 | 159 | 0.8\n",
      "164 | 160 | 0.64\n",
      "164 | 163 | 0.92\n",
      "165 | 21 | 0.59\n",
      "165 | 22 | 0.58\n",
      "165 | 25 | 0.55\n",
      "165 | 26 | 0.74\n",
      "165 | 27 | 0.55\n",
      "165 | 159 | 0.79\n",
      "165 | 160 | 0.62\n",
      "165 | 163 | 0.9\n",
      "165 | 164 | 0.96\n",
      "168 | 167 | 0.64\n",
      "169 | 25 | 0.67\n",
      "169 | 27 | 0.69\n",
      "169 | 31 | 0.64\n",
      "173 | 172 | 0.51\n",
      "174 | 172 | 1.0\n",
      "174 | 173 | 0.51\n",
      "183 | 55 | 0.51\n",
      "185 | 184 | 0.71\n",
      "187 | 183 | 0.53\n",
      "187 | 184 | 0.65\n",
      "187 | 185 | 0.83\n",
      "188 | 47 | 0.65\n",
      "188 | 51 | 0.64\n",
      "196 | 62 | 0.52\n",
      "196 | 63 | 0.59\n",
      "196 | 67 | 0.86\n",
      "197 | 62 | 0.52\n",
      "197 | 63 | 0.58\n",
      "197 | 67 | 0.86\n",
      "197 | 196 | 0.9\n",
      "198 | 62 | 0.51\n",
      "198 | 67 | 0.68\n",
      "198 | 196 | 0.7\n",
      "198 | 197 | 0.72\n",
      "199 | 62 | 0.51\n",
      "199 | 63 | 0.51\n",
      "199 | 67 | 0.81\n",
      "199 | 196 | 0.94\n",
      "199 | 197 | 0.83\n",
      "199 | 198 | 0.71\n",
      "200 | 67 | 0.55\n",
      "200 | 196 | 0.59\n",
      "200 | 197 | 0.57\n",
      "200 | 198 | 0.51\n",
      "200 | 199 | 0.56\n",
      "201 | 74 | 0.59\n",
      "201 | 196 | 0.54\n",
      "201 | 199 | 0.51\n",
      "202 | 74 | 0.83\n",
      "202 | 196 | 0.7\n",
      "202 | 197 | 0.57\n",
      "202 | 198 | 0.52\n",
      "202 | 199 | 0.65\n",
      "202 | 200 | 0.67\n",
      "202 | 201 | 0.8\n",
      "203 | 67 | 0.63\n",
      "203 | 74 | 0.68\n",
      "203 | 196 | 0.81\n",
      "203 | 197 | 0.71\n",
      "203 | 198 | 0.58\n",
      "203 | 199 | 0.8\n",
      "203 | 200 | 0.76\n",
      "203 | 201 | 0.56\n",
      "203 | 202 | 0.84\n",
      "204 | 63 | 0.51\n",
      "204 | 67 | 0.9\n",
      "204 | 196 | 0.87\n",
      "204 | 197 | 0.82\n",
      "204 | 198 | 0.66\n",
      "204 | 199 | 0.83\n",
      "204 | 200 | 0.67\n",
      "204 | 202 | 0.69\n",
      "204 | 203 | 0.8\n",
      "205 | 59 | 0.5\n",
      "205 | 62 | 0.51\n",
      "205 | 63 | 0.61\n",
      "205 | 67 | 0.87\n",
      "205 | 196 | 0.86\n",
      "205 | 197 | 0.84\n",
      "205 | 198 | 0.71\n",
      "205 | 199 | 0.8\n",
      "205 | 202 | 0.52\n",
      "205 | 203 | 0.68\n",
      "205 | 204 | 0.83\n",
      "206 | 74 | 1.0\n",
      "206 | 201 | 0.59\n",
      "206 | 202 | 0.83\n",
      "206 | 203 | 0.68\n",
      "207 | 62 | 0.54\n",
      "207 | 63 | 0.58\n",
      "207 | 67 | 0.86\n",
      "207 | 196 | 0.92\n",
      "207 | 197 | 0.87\n",
      "207 | 198 | 0.68\n",
      "207 | 199 | 0.88\n",
      "207 | 200 | 0.71\n",
      "207 | 202 | 0.67\n",
      "207 | 203 | 0.86\n",
      "207 | 204 | 0.87\n",
      "207 | 205 | 0.87\n",
      "209 | 74 | 1.0\n",
      "209 | 201 | 0.59\n",
      "209 | 202 | 0.83\n",
      "209 | 203 | 0.68\n",
      "209 | 206 | 1.0\n",
      "222 | 87 | 0.67\n",
      "224 | 89 | 0.72\n",
      "225 | 158 | 0.91\n",
      "244 | 109 | 0.96\n",
      "245 | 109 | 0.9\n",
      "245 | 244 | 0.97\n",
      "246 | 109 | 0.95\n",
      "246 | 244 | 0.99\n",
      "246 | 245 | 0.97\n",
      "248 | 114 | 0.7\n",
      "249 | 114 | 0.98\n",
      "249 | 248 | 0.73\n",
      "250 | 158 | 0.87\n",
      "250 | 225 | 0.68\n",
      "251 | 116 | 0.5\n",
      "252 | 117 | 0.99\n",
      "254 | 119 | 0.8\n",
      "268 | 267 | 0.56\n",
      "270 | 135 | 0.95\n",
      "271 | 136 | 0.97\n",
      "271 | 137 | 0.69\n",
      "272 | 136 | 0.67\n",
      "272 | 137 | 0.98\n",
      "272 | 271 | 0.71\n",
      "273 | 138 | 0.92\n",
      "274 | 139 | 0.99\n",
      "275 | 4 | 1.0\n",
      "275 | 7 | 0.92\n",
      "275 | 140 | 1.0\n",
      "275 | 142 | 0.67\n",
      "277 | 4 | 0.72\n",
      "277 | 7 | 0.55\n",
      "277 | 140 | 0.72\n",
      "277 | 142 | 0.97\n",
      "277 | 275 | 0.72\n",
      "278 | 143 | 0.91\n",
      "279 | 144 | 0.98\n",
      "280 | 145 | 0.96\n",
      "281 | 145 | 0.55\n",
      "281 | 146 | 0.95\n",
      "282 | 12 | 0.55\n",
      "282 | 16 | 0.88\n",
      "282 | 147 | 1.0\n",
      "282 | 148 | 0.89\n",
      "282 | 152 | 0.89\n",
      "282 | 154 | 0.8\n",
      "283 | 12 | 0.56\n",
      "283 | 16 | 0.97\n",
      "283 | 147 | 0.89\n",
      "283 | 148 | 1.0\n",
      "283 | 152 | 0.99\n",
      "283 | 154 | 0.94\n",
      "283 | 282 | 0.89\n",
      "285 | 14 | 0.63\n",
      "285 | 150 | 0.97\n",
      "286 | 74 | 0.53\n",
      "286 | 151 | 0.99\n",
      "286 | 206 | 0.53\n",
      "286 | 209 | 0.53\n",
      "287 | 12 | 0.56\n",
      "287 | 16 | 0.98\n",
      "287 | 147 | 0.9\n",
      "287 | 148 | 0.99\n",
      "287 | 152 | 1.0\n",
      "287 | 154 | 0.89\n",
      "287 | 282 | 0.89\n",
      "287 | 283 | 0.99\n",
      "288 | 153 | 1.0\n",
      "289 | 12 | 0.53\n",
      "289 | 16 | 0.88\n",
      "289 | 147 | 0.81\n",
      "289 | 148 | 0.94\n",
      "289 | 152 | 0.89\n",
      "289 | 154 | 0.99\n",
      "289 | 282 | 0.81\n",
      "289 | 283 | 0.94\n",
      "289 | 287 | 0.89\n",
      "290 | 19 | 0.81\n",
      "290 | 155 | 0.95\n",
      "291 | 156 | 0.99\n",
      "292 | 85 | 0.84\n",
      "292 | 157 | 0.99\n",
      "293 | 85 | 0.8\n",
      "293 | 158 | 0.96\n",
      "293 | 225 | 0.91\n",
      "293 | 250 | 0.85\n",
      "294 | 21 | 0.56\n",
      "294 | 22 | 0.5\n",
      "294 | 26 | 0.6\n",
      "294 | 159 | 0.99\n",
      "294 | 160 | 0.57\n",
      "294 | 163 | 0.79\n",
      "294 | 164 | 0.83\n",
      "294 | 165 | 0.82\n",
      "295 | 26 | 0.51\n",
      "295 | 159 | 0.57\n",
      "295 | 160 | 1.0\n",
      "295 | 163 | 0.66\n",
      "295 | 164 | 0.67\n",
      "295 | 165 | 0.65\n",
      "295 | 294 | 0.59\n",
      "296 | 161 | 0.99\n",
      "297 | 162 | 0.99\n",
      "298 | 21 | 0.62\n",
      "298 | 22 | 0.57\n",
      "298 | 25 | 0.52\n",
      "298 | 26 | 0.73\n",
      "298 | 159 | 0.77\n",
      "298 | 160 | 0.64\n",
      "298 | 163 | 0.99\n",
      "298 | 164 | 0.94\n",
      "298 | 165 | 0.92\n",
      "298 | 294 | 0.81\n",
      "298 | 295 | 0.67\n",
      "299 | 21 | 0.62\n",
      "299 | 22 | 0.59\n",
      "299 | 25 | 0.58\n",
      "299 | 26 | 0.77\n",
      "299 | 27 | 0.56\n",
      "299 | 159 | 0.8\n",
      "299 | 160 | 0.65\n",
      "299 | 163 | 0.92\n",
      "299 | 164 | 1.0\n",
      "299 | 165 | 0.96\n",
      "299 | 294 | 0.83\n",
      "299 | 295 | 0.68\n",
      "299 | 298 | 0.95\n",
      "300 | 21 | 0.59\n",
      "300 | 22 | 0.58\n",
      "300 | 25 | 0.56\n",
      "300 | 26 | 0.75\n",
      "300 | 27 | 0.55\n",
      "300 | 159 | 0.79\n",
      "300 | 160 | 0.63\n",
      "300 | 163 | 0.9\n",
      "300 | 164 | 0.97\n",
      "300 | 165 | 1.0\n",
      "300 | 294 | 0.82\n",
      "300 | 295 | 0.66\n",
      "300 | 298 | 0.93\n",
      "300 | 299 | 0.97\n",
      "301 | 166 | 0.96\n",
      "302 | 167 | 0.98\n",
      "302 | 168 | 0.64\n",
      "303 | 167 | 0.64\n",
      "303 | 168 | 0.96\n",
      "303 | 302 | 0.65\n",
      "304 | 25 | 0.69\n",
      "304 | 27 | 0.71\n",
      "304 | 31 | 0.66\n",
      "304 | 169 | 0.98\n",
      "305 | 170 | 0.96\n",
      "306 | 171 | 0.99\n",
      "307 | 172 | 0.96\n",
      "307 | 173 | 0.51\n",
      "307 | 174 | 0.96\n",
      "308 | 172 | 0.55\n",
      "308 | 173 | 0.96\n",
      "308 | 174 | 0.55\n",
      "308 | 307 | 0.53\n",
      "309 | 172 | 0.96\n",
      "309 | 173 | 0.51\n",
      "309 | 174 | 0.96\n",
      "309 | 307 | 1.0\n",
      "309 | 308 | 0.53\n",
      "310 | 175 | 0.96\n",
      "311 | 176 | 0.98\n",
      "312 | 177 | 1.0\n",
      "316 | 180 | 0.88\n",
      "317 | 181 | 0.96\n",
      "318 | 182 | 0.98\n",
      "319 | 55 | 0.56\n",
      "319 | 183 | 0.98\n",
      "319 | 187 | 0.51\n",
      "320 | 184 | 0.99\n",
      "320 | 185 | 0.72\n",
      "320 | 187 | 0.66\n",
      "321 | 184 | 0.71\n",
      "321 | 185 | 0.99\n",
      "321 | 187 | 0.83\n",
      "321 | 320 | 0.72\n",
      "323 | 183 | 0.56\n",
      "323 | 184 | 0.63\n",
      "323 | 185 | 0.82\n",
      "323 | 187 | 0.99\n",
      "323 | 319 | 0.54\n",
      "323 | 320 | 0.64\n",
      "323 | 321 | 0.82\n",
      "324 | 47 | 0.66\n",
      "324 | 51 | 0.65\n",
      "324 | 188 | 0.98\n",
      "331 | 195 | 0.95\n",
      "332 | 62 | 0.57\n",
      "332 | 63 | 0.66\n",
      "332 | 67 | 0.88\n",
      "332 | 196 | 0.96\n",
      "332 | 197 | 0.91\n",
      "332 | 198 | 0.66\n",
      "332 | 199 | 0.9\n",
      "332 | 202 | 0.56\n",
      "332 | 203 | 0.72\n",
      "332 | 204 | 0.82\n",
      "332 | 205 | 0.89\n",
      "332 | 207 | 0.91\n",
      "333 | 62 | 0.52\n",
      "333 | 63 | 0.59\n",
      "333 | 67 | 0.87\n",
      "333 | 196 | 0.87\n",
      "333 | 197 | 0.98\n",
      "333 | 198 | 0.74\n",
      "333 | 199 | 0.8\n",
      "333 | 200 | 0.56\n",
      "333 | 202 | 0.52\n",
      "333 | 203 | 0.68\n",
      "333 | 204 | 0.79\n",
      "333 | 205 | 0.85\n",
      "333 | 207 | 0.87\n",
      "333 | 332 | 0.9\n",
      "334 | 62 | 0.56\n",
      "334 | 67 | 0.75\n",
      "334 | 196 | 0.75\n",
      "334 | 197 | 0.78\n",
      "334 | 198 | 0.99\n",
      "334 | 199 | 0.75\n",
      "334 | 200 | 0.51\n",
      "334 | 202 | 0.51\n",
      "334 | 203 | 0.6\n",
      "334 | 204 | 0.7\n",
      "334 | 205 | 0.76\n",
      "334 | 207 | 0.73\n",
      "334 | 332 | 0.73\n",
      "334 | 333 | 0.8\n",
      "335 | 62 | 0.57\n",
      "335 | 63 | 0.61\n",
      "335 | 67 | 0.85\n",
      "335 | 196 | 0.93\n",
      "335 | 197 | 0.86\n",
      "335 | 198 | 0.69\n",
      "335 | 199 | 0.96\n",
      "335 | 202 | 0.54\n",
      "335 | 203 | 0.72\n",
      "335 | 204 | 0.79\n",
      "335 | 205 | 0.85\n",
      "335 | 207 | 0.9\n",
      "335 | 332 | 0.96\n",
      "335 | 333 | 0.86\n",
      "335 | 334 | 0.74\n",
      "336 | 62 | 0.57\n",
      "336 | 63 | 0.6\n",
      "336 | 67 | 0.87\n",
      "336 | 196 | 0.91\n",
      "336 | 197 | 0.9\n",
      "336 | 198 | 0.7\n",
      "336 | 199 | 0.88\n",
      "336 | 200 | 0.52\n",
      "336 | 202 | 0.52\n",
      "336 | 203 | 0.71\n",
      "336 | 204 | 0.82\n",
      "336 | 205 | 0.88\n",
      "336 | 207 | 0.9\n",
      "336 | 332 | 0.94\n",
      "336 | 333 | 0.9\n",
      "336 | 334 | 0.76\n",
      "336 | 335 | 0.93\n",
      "337 | 74 | 0.55\n",
      "337 | 196 | 0.64\n",
      "337 | 198 | 0.53\n",
      "337 | 199 | 0.6\n",
      "337 | 200 | 0.6\n",
      "337 | 201 | 0.93\n",
      "337 | 202 | 0.81\n",
      "337 | 203 | 0.65\n",
      "337 | 204 | 0.56\n",
      "337 | 206 | 0.55\n",
      "337 | 207 | 0.51\n",
      "337 | 209 | 0.55\n",
      "337 | 334 | 0.51\n",
      "338 | 74 | 0.87\n",
      "338 | 196 | 0.69\n",
      "338 | 197 | 0.57\n",
      "338 | 199 | 0.65\n",
      "338 | 200 | 0.68\n",
      "338 | 201 | 0.75\n",
      "338 | 202 | 0.99\n",
      "338 | 203 | 0.86\n",
      "338 | 204 | 0.7\n",
      "338 | 205 | 0.52\n",
      "338 | 206 | 0.87\n",
      "338 | 207 | 0.68\n",
      "338 | 209 | 0.87\n",
      "338 | 332 | 0.56\n",
      "338 | 333 | 0.53\n",
      "338 | 335 | 0.54\n",
      "338 | 336 | 0.52\n",
      "338 | 337 | 0.76\n",
      "339 | 67 | 0.54\n",
      "339 | 74 | 0.78\n",
      "339 | 196 | 0.76\n",
      "339 | 197 | 0.64\n",
      "339 | 198 | 0.5\n",
      "339 | 199 | 0.76\n",
      "339 | 200 | 0.73\n",
      "339 | 201 | 0.59\n",
      "339 | 202 | 0.88\n",
      "339 | 203 | 0.98\n",
      "339 | 204 | 0.75\n",
      "339 | 205 | 0.61\n",
      "339 | 206 | 0.78\n",
      "339 | 207 | 0.82\n",
      "339 | 209 | 0.78\n",
      "339 | 332 | 0.66\n",
      "339 | 333 | 0.6\n",
      "339 | 334 | 0.52\n",
      "339 | 335 | 0.67\n",
      "339 | 336 | 0.65\n",
      "339 | 337 | 0.64\n",
      "339 | 338 | 0.91\n",
      "340 | 63 | 0.52\n",
      "340 | 67 | 0.95\n",
      "340 | 196 | 0.85\n",
      "340 | 197 | 0.83\n",
      "340 | 198 | 0.68\n",
      "340 | 199 | 0.81\n",
      "340 | 200 | 0.63\n",
      "340 | 202 | 0.58\n",
      "340 | 203 | 0.72\n",
      "340 | 204 | 0.99\n",
      "340 | 205 | 0.84\n",
      "340 | 207 | 0.85\n",
      "340 | 332 | 0.82\n",
      "340 | 333 | 0.82\n",
      "340 | 334 | 0.73\n",
      "340 | 335 | 0.79\n",
      "340 | 336 | 0.82\n",
      "340 | 338 | 0.59\n",
      "340 | 339 | 0.66\n",
      "341 | 62 | 0.51\n",
      "341 | 63 | 0.6\n",
      "341 | 67 | 0.91\n",
      "341 | 196 | 0.86\n",
      "341 | 197 | 0.85\n",
      "341 | 198 | 0.72\n",
      "341 | 199 | 0.81\n",
      "341 | 200 | 0.51\n",
      "341 | 202 | 0.51\n",
      "341 | 203 | 0.67\n",
      "341 | 204 | 0.87\n",
      "341 | 205 | 0.99\n",
      "341 | 207 | 0.87\n",
      "341 | 332 | 0.89\n",
      "341 | 333 | 0.86\n",
      "341 | 334 | 0.78\n",
      "341 | 335 | 0.85\n",
      "341 | 336 | 0.89\n",
      "341 | 338 | 0.52\n",
      "341 | 339 | 0.61\n",
      "341 | 340 | 0.89\n",
      "342 | 74 | 1.0\n",
      "342 | 201 | 0.59\n",
      "342 | 202 | 0.83\n",
      "342 | 203 | 0.68\n",
      "342 | 206 | 1.0\n",
      "342 | 209 | 1.0\n",
      "342 | 286 | 0.53\n",
      "342 | 337 | 0.55\n",
      "342 | 338 | 0.87\n",
      "342 | 339 | 0.78\n",
      "343 | 62 | 0.58\n",
      "343 | 63 | 0.61\n",
      "343 | 67 | 0.87\n",
      "343 | 196 | 0.9\n",
      "343 | 197 | 0.88\n",
      "343 | 198 | 0.67\n",
      "343 | 199 | 0.87\n",
      "343 | 200 | 0.64\n",
      "343 | 202 | 0.57\n",
      "343 | 203 | 0.8\n",
      "343 | 204 | 0.83\n",
      "343 | 205 | 0.88\n",
      "343 | 207 | 0.98\n",
      "343 | 332 | 0.94\n",
      "343 | 333 | 0.89\n",
      "343 | 334 | 0.73\n",
      "343 | 335 | 0.92\n",
      "343 | 336 | 0.92\n",
      "343 | 338 | 0.59\n",
      "343 | 339 | 0.75\n",
      "343 | 340 | 0.82\n",
      "343 | 341 | 0.88\n",
      "344 | 208 | 0.96\n",
      "345 | 67 | 0.69\n",
      "345 | 72 | 0.65\n",
      "345 | 73 | 0.83\n",
      "345 | 196 | 0.64\n",
      "345 | 197 | 0.64\n",
      "345 | 198 | 0.55\n",
      "345 | 199 | 0.63\n",
      "345 | 203 | 0.52\n",
      "345 | 204 | 0.67\n",
      "345 | 205 | 0.7\n",
      "345 | 207 | 0.67\n",
      "345 | 332 | 0.66\n",
      "345 | 333 | 0.63\n",
      "345 | 334 | 0.58\n",
      "345 | 335 | 0.66\n",
      "345 | 336 | 0.69\n",
      "345 | 340 | 0.68\n",
      "345 | 341 | 0.71\n",
      "345 | 343 | 0.67\n",
      "346 | 67 | 0.63\n",
      "346 | 72 | 0.69\n",
      "346 | 73 | 0.84\n",
      "346 | 196 | 0.59\n",
      "346 | 197 | 0.58\n",
      "346 | 198 | 0.51\n",
      "346 | 199 | 0.57\n",
      "346 | 203 | 0.53\n",
      "346 | 204 | 0.66\n",
      "346 | 205 | 0.63\n",
      "346 | 207 | 0.61\n",
      "346 | 332 | 0.58\n",
      "346 | 333 | 0.58\n",
      "346 | 334 | 0.54\n",
      "346 | 335 | 0.57\n",
      "346 | 336 | 0.61\n",
      "346 | 340 | 0.66\n",
      "346 | 341 | 0.65\n",
      "346 | 343 | 0.6\n",
      "346 | 345 | 0.98\n",
      "347 | 74 | 1.0\n",
      "347 | 201 | 0.59\n",
      "347 | 202 | 0.83\n",
      "347 | 203 | 0.68\n",
      "347 | 206 | 1.0\n",
      "347 | 209 | 1.0\n",
      "347 | 286 | 0.53\n",
      "347 | 337 | 0.55\n",
      "347 | 338 | 0.87\n",
      "347 | 339 | 0.78\n",
      "347 | 342 | 1.0\n",
      "348 | 210 | 0.95\n",
      "349 | 211 | 0.99\n",
      "350 | 212 | 0.99\n",
      "351 | 213 | 1.0\n",
      "352 | 214 | 0.98\n",
      "353 | 215 | 0.98\n",
      "354 | 216 | 0.97\n",
      "355 | 217 | 0.99\n",
      "356 | 218 | 0.95\n",
      "357 | 219 | 0.98\n",
      "358 | 220 | 0.87\n",
      "358 | 293 | 0.59\n",
      "359 | 221 | 0.98\n",
      "360 | 87 | 0.69\n",
      "360 | 222 | 0.99\n",
      "361 | 223 | 0.98\n",
      "362 | 89 | 0.71\n",
      "362 | 224 | 1.0\n",
      "363 | 158 | 0.87\n",
      "363 | 225 | 0.96\n",
      "363 | 250 | 0.65\n",
      "363 | 293 | 0.93\n",
      "365 | 227 | 0.97\n",
      "365 | 228 | 0.51\n",
      "366 | 227 | 0.54\n",
      "366 | 228 | 0.97\n",
      "366 | 365 | 0.56\n",
      "367 | 227 | 0.74\n",
      "367 | 228 | 0.64\n",
      "367 | 365 | 0.74\n",
      "367 | 366 | 0.64\n",
      "368 | 227 | 0.6\n",
      "368 | 228 | 0.8\n",
      "368 | 365 | 0.61\n",
      "368 | 366 | 0.77\n",
      "368 | 367 | 0.79\n",
      "376 | 238 | 0.97\n",
      "377 | 239 | 0.95\n",
      "382 | 109 | 0.96\n",
      "382 | 244 | 1.0\n",
      "382 | 245 | 0.97\n",
      "382 | 246 | 0.99\n",
      "383 | 109 | 0.9\n",
      "383 | 244 | 0.97\n",
      "383 | 245 | 1.0\n",
      "383 | 246 | 0.98\n",
      "383 | 382 | 0.97\n",
      "384 | 109 | 0.95\n",
      "384 | 244 | 0.99\n",
      "384 | 245 | 0.97\n",
      "384 | 246 | 1.0\n",
      "384 | 382 | 0.99\n",
      "384 | 383 | 0.98\n",
      "385 | 247 | 0.87\n",
      "386 | 114 | 0.7\n",
      "386 | 248 | 1.0\n",
      "386 | 249 | 0.73\n",
      "387 | 114 | 0.98\n",
      "387 | 248 | 0.73\n",
      "387 | 249 | 1.0\n",
      "387 | 386 | 0.73\n",
      "388 | 158 | 0.86\n",
      "388 | 225 | 0.68\n",
      "388 | 250 | 0.97\n",
      "388 | 293 | 0.9\n",
      "388 | 363 | 0.7\n",
      "389 | 251 | 1.0\n",
      "390 | 117 | 0.99\n",
      "390 | 252 | 1.0\n",
      "391 | 253 | 0.99\n",
      "392 | 119 | 0.79\n",
      "392 | 254 | 0.99\n",
      "393 | 255 | 0.99\n",
      "405 | 267 | 0.99\n",
      "405 | 268 | 0.57\n",
      "406 | 267 | 0.61\n",
      "406 | 268 | 0.97\n",
      "406 | 405 | 0.61\n",
      "407 | 269 | 0.95\n",
      "408 | 135 | 1.0\n",
      "408 | 270 | 0.94\n",
      "409 | 136 | 1.0\n",
      "409 | 137 | 0.66\n",
      "409 | 271 | 0.97\n",
      "409 | 272 | 0.67\n",
      "410 | 136 | 0.66\n",
      "410 | 137 | 1.0\n",
      "410 | 271 | 0.68\n",
      "410 | 272 | 0.97\n",
      "410 | 409 | 0.65\n",
      "411 | 138 | 1.0\n",
      "411 | 273 | 0.92\n",
      "412 | 139 | 0.85\n",
      "412 | 274 | 0.82\n",
      "413 | 4 | 0.94\n",
      "413 | 7 | 0.79\n",
      "413 | 140 | 0.94\n",
      "413 | 142 | 0.63\n",
      "413 | 275 | 0.94\n",
      "413 | 277 | 0.67\n",
      "415 | 4 | 0.68\n",
      "415 | 7 | 0.56\n",
      "415 | 140 | 0.68\n",
      "415 | 142 | 0.99\n",
      "415 | 275 | 0.68\n",
      "415 | 277 | 0.97\n",
      "415 | 413 | 0.65\n",
      "416 | 143 | 1.0\n",
      "416 | 278 | 0.91\n",
      "417 | 144 | 0.99\n",
      "417 | 279 | 0.97\n",
      "420 | 12 | 0.55\n",
      "420 | 16 | 0.9\n",
      "420 | 147 | 1.0\n",
      "420 | 148 | 0.9\n",
      "420 | 152 | 0.91\n",
      "420 | 154 | 0.81\n",
      "420 | 282 | 1.0\n",
      "420 | 283 | 0.9\n",
      "420 | 287 | 0.91\n",
      "420 | 289 | 0.82\n",
      "421 | 12 | 0.55\n",
      "421 | 16 | 0.96\n",
      "421 | 147 | 0.89\n",
      "421 | 148 | 1.0\n",
      "421 | 152 | 0.98\n",
      "421 | 154 | 0.95\n",
      "421 | 282 | 0.88\n",
      "421 | 283 | 1.0\n",
      "421 | 287 | 0.98\n",
      "421 | 289 | 0.95\n",
      "421 | 420 | 0.9\n",
      "423 | 150 | 0.57\n",
      "423 | 285 | 0.56\n",
      "424 | 151 | 0.98\n",
      "424 | 286 | 0.97\n",
      "425 | 12 | 0.54\n",
      "425 | 16 | 0.94\n",
      "425 | 147 | 0.87\n",
      "425 | 148 | 0.96\n",
      "425 | 152 | 0.98\n",
      "425 | 154 | 0.86\n",
      "425 | 282 | 0.87\n",
      "425 | 283 | 0.96\n",
      "425 | 287 | 0.97\n",
      "425 | 289 | 0.86\n",
      "425 | 420 | 0.88\n",
      "425 | 421 | 0.95\n",
      "426 | 17 | 0.5\n",
      "426 | 153 | 1.0\n",
      "426 | 288 | 0.99\n",
      "427 | 12 | 0.51\n",
      "427 | 16 | 0.89\n",
      "427 | 147 | 0.82\n",
      "427 | 148 | 0.95\n",
      "427 | 152 | 0.91\n",
      "427 | 154 | 1.0\n",
      "427 | 282 | 0.82\n",
      "427 | 283 | 0.95\n",
      "427 | 287 | 0.91\n",
      "427 | 289 | 0.99\n",
      "427 | 420 | 0.83\n",
      "427 | 421 | 0.97\n",
      "427 | 425 | 0.88\n",
      "428 | 19 | 0.85\n",
      "428 | 155 | 1.0\n",
      "428 | 290 | 0.96\n",
      "429 | 156 | 1.0\n",
      "429 | 291 | 0.99\n",
      "430 | 21 | 0.67\n",
      "430 | 22 | 0.62\n",
      "430 | 25 | 0.52\n",
      "430 | 26 | 0.74\n",
      "430 | 159 | 0.87\n",
      "430 | 160 | 0.57\n",
      "430 | 163 | 0.83\n",
      "430 | 164 | 0.88\n",
      "430 | 165 | 0.85\n",
      "430 | 294 | 0.89\n",
      "430 | 295 | 0.6\n",
      "430 | 298 | 0.84\n",
      "430 | 299 | 0.87\n",
      "430 | 300 | 0.85\n",
      "431 | 21 | 0.6\n",
      "431 | 25 | 0.58\n",
      "431 | 26 | 0.74\n",
      "431 | 27 | 0.54\n",
      "431 | 159 | 0.69\n",
      "431 | 160 | 0.81\n",
      "431 | 163 | 0.81\n",
      "431 | 164 | 0.85\n",
      "431 | 165 | 0.81\n",
      "431 | 294 | 0.72\n",
      "431 | 295 | 0.83\n",
      "431 | 298 | 0.83\n",
      "431 | 299 | 0.85\n",
      "431 | 300 | 0.82\n",
      "431 | 430 | 0.9\n",
      "434 | 21 | 0.63\n",
      "434 | 22 | 0.62\n",
      "434 | 25 | 0.61\n",
      "434 | 26 | 0.82\n",
      "434 | 27 | 0.57\n",
      "434 | 159 | 0.71\n",
      "434 | 160 | 0.58\n",
      "434 | 163 | 0.88\n",
      "434 | 164 | 0.9\n",
      "434 | 165 | 0.86\n",
      "434 | 294 | 0.75\n",
      "434 | 295 | 0.61\n",
      "434 | 298 | 0.89\n",
      "434 | 299 | 0.89\n",
      "434 | 300 | 0.86\n",
      "434 | 430 | 0.95\n",
      "434 | 431 | 0.93\n",
      "435 | 21 | 0.64\n",
      "435 | 22 | 0.64\n",
      "435 | 25 | 0.6\n",
      "435 | 26 | 0.83\n",
      "435 | 27 | 0.57\n",
      "435 | 159 | 0.71\n",
      "435 | 160 | 0.57\n",
      "435 | 163 | 0.84\n",
      "435 | 164 | 0.91\n",
      "435 | 165 | 0.87\n",
      "435 | 294 | 0.75\n",
      "435 | 295 | 0.6\n",
      "435 | 298 | 0.86\n",
      "435 | 299 | 0.9\n",
      "435 | 300 | 0.86\n",
      "435 | 430 | 0.95\n",
      "435 | 431 | 0.93\n",
      "435 | 434 | 0.99\n",
      "436 | 21 | 0.64\n",
      "436 | 22 | 0.63\n",
      "436 | 25 | 0.61\n",
      "436 | 26 | 0.81\n",
      "436 | 27 | 0.57\n",
      "436 | 159 | 0.71\n",
      "436 | 160 | 0.58\n",
      "436 | 163 | 0.84\n",
      "436 | 164 | 0.9\n",
      "436 | 165 | 0.88\n",
      "436 | 294 | 0.75\n",
      "436 | 295 | 0.61\n",
      "436 | 298 | 0.86\n",
      "436 | 299 | 0.9\n",
      "436 | 300 | 0.87\n",
      "436 | 430 | 0.95\n",
      "436 | 431 | 0.93\n",
      "436 | 434 | 0.99\n",
      "436 | 435 | 1.0\n",
      "437 | 166 | 0.99\n",
      "437 | 301 | 0.95\n",
      "438 | 167 | 0.68\n",
      "438 | 302 | 0.66\n",
      "439 | 29 | 0.52\n",
      "439 | 167 | 0.64\n",
      "439 | 168 | 0.79\n",
      "439 | 302 | 0.66\n",
      "439 | 303 | 0.77\n",
      "439 | 438 | 0.67\n",
      "440 | 25 | 0.69\n",
      "440 | 27 | 0.71\n",
      "440 | 31 | 0.7\n",
      "440 | 169 | 1.0\n",
      "440 | 304 | 0.98\n",
      "441 | 170 | 0.99\n",
      "441 | 305 | 0.95\n",
      "442 | 171 | 0.97\n",
      "442 | 306 | 0.96\n",
      "443 | 172 | 1.0\n",
      "443 | 173 | 0.52\n",
      "443 | 174 | 1.0\n",
      "443 | 307 | 0.96\n",
      "443 | 308 | 0.55\n",
      "443 | 309 | 0.96\n",
      "444 | 172 | 0.53\n",
      "444 | 173 | 0.99\n",
      "444 | 174 | 0.53\n",
      "444 | 307 | 0.53\n",
      "444 | 308 | 0.95\n",
      "444 | 309 | 0.53\n",
      "444 | 443 | 0.54\n",
      "445 | 172 | 1.0\n",
      "445 | 173 | 0.5\n",
      "445 | 174 | 1.0\n",
      "445 | 307 | 0.96\n",
      "445 | 308 | 0.54\n",
      "445 | 309 | 0.96\n",
      "445 | 443 | 0.99\n",
      "445 | 444 | 0.51\n",
      "446 | 175 | 1.0\n",
      "446 | 310 | 0.95\n",
      "447 | 176 | 1.0\n",
      "447 | 311 | 0.98\n",
      "448 | 177 | 1.0\n",
      "448 | 312 | 1.0\n",
      "452 | 180 | 0.99\n",
      "452 | 316 | 0.86\n",
      "453 | 181 | 1.0\n",
      "453 | 317 | 0.96\n",
      "454 | 182 | 0.99\n",
      "454 | 318 | 0.97\n",
      "455 | 55 | 0.53\n",
      "455 | 183 | 1.0\n",
      "455 | 187 | 0.53\n",
      "455 | 319 | 0.98\n",
      "455 | 323 | 0.56\n",
      "456 | 184 | 0.97\n",
      "456 | 185 | 0.71\n",
      "456 | 187 | 0.64\n",
      "456 | 320 | 0.96\n",
      "456 | 321 | 0.72\n",
      "456 | 323 | 0.62\n",
      "457 | 184 | 0.69\n",
      "457 | 185 | 1.0\n",
      "457 | 187 | 0.81\n",
      "457 | 320 | 0.7\n",
      "457 | 321 | 0.99\n",
      "457 | 323 | 0.8\n",
      "457 | 456 | 0.71\n",
      "459 | 183 | 0.54\n",
      "459 | 184 | 0.65\n",
      "459 | 185 | 0.82\n",
      "459 | 187 | 1.0\n",
      "459 | 319 | 0.52\n",
      "459 | 320 | 0.66\n",
      "459 | 321 | 0.82\n",
      "459 | 323 | 0.99\n",
      "459 | 455 | 0.54\n",
      "459 | 456 | 0.64\n",
      "459 | 457 | 0.81\n",
      "460 | 320 | 0.5\n",
      "460 | 456 | 0.51\n",
      "467 | 195 | 1.0\n",
      "467 | 331 | 0.95\n",
      "469 | 62 | 0.52\n",
      "469 | 63 | 0.59\n",
      "469 | 67 | 0.87\n",
      "469 | 196 | 0.9\n",
      "469 | 197 | 1.0\n",
      "469 | 198 | 0.72\n",
      "469 | 199 | 0.83\n",
      "469 | 200 | 0.56\n",
      "469 | 202 | 0.55\n",
      "469 | 203 | 0.71\n",
      "469 | 204 | 0.81\n",
      "469 | 205 | 0.85\n",
      "469 | 207 | 0.88\n",
      "469 | 332 | 0.92\n",
      "469 | 333 | 0.99\n",
      "469 | 334 | 0.77\n",
      "469 | 335 | 0.88\n",
      "469 | 336 | 0.91\n",
      "469 | 338 | 0.56\n",
      "469 | 339 | 0.64\n",
      "469 | 340 | 0.82\n",
      "469 | 341 | 0.86\n",
      "469 | 343 | 0.89\n",
      "469 | 345 | 0.64\n",
      "469 | 346 | 0.58\n",
      "470 | 62 | 0.53\n",
      "470 | 67 | 0.64\n",
      "470 | 196 | 0.68\n",
      "470 | 197 | 0.7\n",
      "470 | 198 | 1.0\n",
      "470 | 199 | 0.7\n",
      "470 | 202 | 0.51\n",
      "470 | 203 | 0.56\n",
      "470 | 204 | 0.62\n",
      "470 | 205 | 0.69\n",
      "470 | 207 | 0.66\n",
      "470 | 332 | 0.65\n",
      "470 | 333 | 0.71\n",
      "470 | 334 | 0.98\n",
      "470 | 335 | 0.67\n",
      "470 | 336 | 0.68\n",
      "470 | 337 | 0.51\n",
      "470 | 340 | 0.64\n",
      "470 | 341 | 0.7\n",
      "470 | 343 | 0.65\n",
      "470 | 345 | 0.53\n",
      "470 | 469 | 0.69\n",
      "471 | 67 | 0.62\n",
      "471 | 74 | 0.51\n",
      "471 | 196 | 0.83\n",
      "471 | 197 | 0.69\n",
      "471 | 198 | 0.67\n",
      "471 | 199 | 0.94\n",
      "471 | 200 | 0.59\n",
      "471 | 201 | 0.64\n",
      "471 | 202 | 0.73\n",
      "471 | 203 | 0.8\n",
      "471 | 204 | 0.74\n",
      "471 | 205 | 0.63\n",
      "471 | 206 | 0.51\n",
      "471 | 207 | 0.74\n",
      "471 | 209 | 0.51\n",
      "471 | 332 | 0.72\n",
      "471 | 333 | 0.64\n",
      "471 | 334 | 0.66\n",
      "471 | 335 | 0.83\n",
      "471 | 336 | 0.72\n",
      "471 | 337 | 0.7\n",
      "471 | 338 | 0.72\n",
      "471 | 339 | 0.79\n",
      "471 | 340 | 0.69\n",
      "471 | 341 | 0.64\n",
      "471 | 342 | 0.51\n",
      "471 | 343 | 0.7\n",
      "471 | 345 | 0.51\n",
      "471 | 347 | 0.51\n",
      "471 | 469 | 0.68\n",
      "471 | 470 | 0.65\n",
      "473 | 201 | 0.87\n",
      "473 | 202 | 0.58\n",
      "473 | 337 | 0.76\n",
      "473 | 338 | 0.52\n",
      "473 | 471 | 0.52\n",
      "474 | 196 | 0.68\n",
      "474 | 197 | 0.55\n",
      "474 | 199 | 0.65\n",
      "474 | 201 | 0.74\n",
      "474 | 202 | 0.71\n",
      "474 | 203 | 0.55\n",
      "474 | 204 | 0.55\n",
      "474 | 205 | 0.5\n",
      "474 | 207 | 0.57\n",
      "474 | 332 | 0.61\n",
      "474 | 333 | 0.51\n",
      "474 | 335 | 0.6\n",
      "474 | 336 | 0.53\n",
      "474 | 337 | 0.71\n",
      "474 | 338 | 0.65\n",
      "474 | 339 | 0.55\n",
      "474 | 340 | 0.52\n",
      "474 | 341 | 0.51\n",
      "474 | 343 | 0.54\n",
      "474 | 469 | 0.54\n",
      "474 | 471 | 0.65\n",
      "474 | 473 | 0.79\n",
      "475 | 67 | 0.59\n",
      "475 | 74 | 0.73\n",
      "475 | 196 | 0.79\n",
      "475 | 197 | 0.67\n",
      "475 | 198 | 0.56\n",
      "475 | 199 | 0.77\n",
      "475 | 200 | 0.76\n",
      "475 | 201 | 0.58\n",
      "475 | 202 | 0.87\n",
      "475 | 203 | 1.0\n",
      "475 | 204 | 0.78\n",
      "475 | 205 | 0.64\n",
      "475 | 206 | 0.73\n",
      "475 | 207 | 0.83\n",
      "475 | 209 | 0.73\n",
      "475 | 332 | 0.68\n",
      "475 | 333 | 0.64\n",
      "475 | 334 | 0.57\n",
      "475 | 335 | 0.68\n",
      "475 | 336 | 0.68\n",
      "475 | 337 | 0.67\n",
      "475 | 338 | 0.88\n",
      "475 | 339 | 0.99\n",
      "475 | 340 | 0.7\n",
      "475 | 341 | 0.64\n",
      "475 | 342 | 0.73\n",
      "475 | 343 | 0.76\n",
      "475 | 346 | 0.5\n",
      "475 | 347 | 0.73\n",
      "475 | 469 | 0.67\n",
      "475 | 470 | 0.54\n",
      "475 | 471 | 0.8\n",
      "475 | 474 | 0.54\n",
      "476 | 198 | 0.51\n",
      "476 | 205 | 0.53\n",
      "476 | 334 | 0.53\n",
      "476 | 340 | 0.52\n",
      "476 | 341 | 0.55\n",
      "476 | 470 | 0.5\n",
      "477 | 59 | 0.51\n",
      "477 | 62 | 0.53\n",
      "477 | 63 | 0.62\n",
      "477 | 67 | 0.92\n",
      "477 | 196 | 0.88\n",
      "477 | 197 | 0.85\n",
      "477 | 198 | 0.7\n",
      "477 | 199 | 0.83\n",
      "477 | 203 | 0.67\n",
      "477 | 204 | 0.87\n",
      "477 | 205 | 0.99\n",
      "477 | 207 | 0.88\n",
      "477 | 332 | 0.91\n",
      "477 | 333 | 0.85\n",
      "477 | 334 | 0.76\n",
      "477 | 335 | 0.87\n",
      "477 | 336 | 0.89\n",
      "477 | 339 | 0.6\n",
      "477 | 340 | 0.88\n",
      "477 | 341 | 0.99\n",
      "477 | 343 | 0.89\n",
      "477 | 345 | 0.7\n",
      "477 | 346 | 0.63\n",
      "477 | 469 | 0.85\n",
      "477 | 470 | 0.68\n",
      "477 | 471 | 0.64\n",
      "477 | 474 | 0.5\n",
      "477 | 475 | 0.63\n",
      "477 | 476 | 0.53\n",
      "478 | 74 | 1.0\n",
      "478 | 201 | 0.59\n",
      "478 | 202 | 0.83\n",
      "478 | 203 | 0.68\n",
      "478 | 206 | 1.0\n",
      "478 | 209 | 1.0\n",
      "478 | 286 | 0.53\n",
      "478 | 337 | 0.55\n",
      "478 | 338 | 0.87\n",
      "478 | 339 | 0.78\n",
      "478 | 342 | 1.0\n",
      "478 | 347 | 1.0\n",
      "478 | 471 | 0.51\n",
      "478 | 475 | 0.73\n",
      "479 | 62 | 0.51\n",
      "479 | 63 | 0.56\n",
      "479 | 67 | 0.85\n",
      "479 | 196 | 0.91\n",
      "479 | 197 | 0.87\n",
      "479 | 198 | 0.69\n",
      "479 | 199 | 0.88\n",
      "479 | 200 | 0.75\n",
      "479 | 202 | 0.69\n",
      "479 | 203 | 0.88\n",
      "479 | 204 | 0.88\n",
      "479 | 205 | 0.86\n",
      "479 | 207 | 1.0\n",
      "479 | 332 | 0.89\n",
      "479 | 333 | 0.86\n",
      "479 | 334 | 0.74\n",
      "479 | 335 | 0.88\n",
      "479 | 336 | 0.89\n",
      "479 | 337 | 0.55\n",
      "479 | 338 | 0.71\n",
      "479 | 339 | 0.84\n",
      "479 | 340 | 0.85\n",
      "479 | 341 | 0.86\n",
      "479 | 343 | 0.97\n",
      "479 | 345 | 0.66\n",
      "479 | 346 | 0.61\n",
      "479 | 469 | 0.87\n",
      "479 | 470 | 0.67\n",
      "479 | 471 | 0.76\n",
      "479 | 474 | 0.57\n",
      "479 | 475 | 0.85\n",
      "479 | 477 | 0.87\n",
      "480 | 208 | 0.8\n",
      "480 | 344 | 0.78\n",
      "480 | 476 | 0.57\n",
      "490 | 83 | 0.6\n",
      "490 | 218 | 0.98\n",
      "490 | 356 | 0.93\n",
      "491 | 219 | 1.0\n",
      "491 | 357 | 0.97\n",
      "492 | 158 | 0.58\n",
      "492 | 220 | 0.99\n",
      "492 | 293 | 0.59\n",
      "492 | 358 | 0.86\n",
      "493 | 221 | 1.0\n",
      "493 | 359 | 0.98\n",
      "494 | 87 | 0.68\n",
      "494 | 222 | 1.0\n",
      "494 | 360 | 1.0\n",
      "495 | 223 | 1.0\n",
      "495 | 361 | 0.97\n",
      "496 | 224 | 0.82\n",
      "496 | 362 | 0.82\n",
      "497 | 158 | 0.9\n",
      "497 | 225 | 0.99\n",
      "497 | 250 | 0.68\n",
      "497 | 293 | 0.91\n",
      "497 | 363 | 0.96\n",
      "497 | 388 | 0.68\n",
      "510 | 103 | 0.61\n",
      "516 | 109 | 0.96\n",
      "516 | 244 | 1.0\n",
      "516 | 245 | 0.97\n",
      "516 | 246 | 0.99\n",
      "516 | 382 | 1.0\n",
      "516 | 383 | 0.97\n",
      "516 | 384 | 0.99\n",
      "517 | 109 | 0.91\n",
      "517 | 244 | 0.98\n",
      "517 | 245 | 1.0\n",
      "517 | 246 | 0.98\n",
      "517 | 382 | 0.98\n",
      "517 | 383 | 1.0\n",
      "517 | 384 | 0.98\n",
      "517 | 516 | 0.98\n",
      "518 | 109 | 0.95\n",
      "518 | 244 | 0.99\n",
      "518 | 245 | 0.97\n",
      "518 | 246 | 1.0\n",
      "518 | 382 | 0.99\n",
      "518 | 383 | 0.98\n",
      "518 | 384 | 1.0\n",
      "518 | 516 | 0.99\n",
      "518 | 517 | 0.98\n",
      "519 | 247 | 0.97\n",
      "519 | 385 | 0.95\n",
      "520 | 114 | 0.7\n",
      "520 | 248 | 1.0\n",
      "520 | 249 | 0.73\n",
      "520 | 386 | 1.0\n",
      "520 | 387 | 0.73\n",
      "521 | 114 | 0.61\n",
      "521 | 249 | 0.6\n",
      "521 | 387 | 0.6\n",
      "522 | 158 | 0.87\n",
      "522 | 225 | 0.71\n",
      "522 | 250 | 0.99\n",
      "522 | 293 | 0.84\n",
      "522 | 363 | 0.68\n",
      "522 | 388 | 0.96\n",
      "522 | 497 | 0.72\n",
      "523 | 251 | 1.0\n",
      "523 | 389 | 1.0\n",
      "524 | 117 | 0.98\n",
      "524 | 252 | 1.0\n",
      "524 | 390 | 1.0\n",
      "525 | 253 | 1.0\n",
      "525 | 391 | 0.99\n",
      "526 | 119 | 0.81\n",
      "526 | 254 | 1.0\n",
      "526 | 392 | 0.99\n",
      "527 | 255 | 1.0\n",
      "527 | 393 | 0.98\n",
      "539 | 267 | 1.0\n",
      "539 | 268 | 0.55\n",
      "539 | 405 | 0.99\n",
      "539 | 406 | 0.6\n",
      "540 | 267 | 0.56\n",
      "540 | 268 | 1.0\n",
      "540 | 405 | 0.56\n",
      "540 | 406 | 0.97\n",
      "540 | 539 | 0.55\n",
      "541 | 269 | 0.97\n",
      "541 | 407 | 0.91\n",
      "542 | 85 | 1.0\n",
      "543 | 220 | 1.0\n",
      "543 | 358 | 0.87\n",
      "543 | 492 | 0.99\n",
      "544 | 220 | 0.87\n",
      "544 | 358 | 1.0\n",
      "544 | 492 | 0.86\n",
      "544 | 543 | 0.63\n",
      "545 | 220 | 0.99\n",
      "545 | 358 | 0.86\n",
      "545 | 492 | 1.0\n",
      "545 | 543 | 0.99\n",
      "545 | 544 | 0.65\n",
      "549 | 546 | 0.73\n",
      "551 | 550 | 0.72\n",
      "552 | 546 | 0.73\n",
      "552 | 549 | 1.0\n",
      "553 | 550 | 0.98\n",
      "553 | 551 | 0.69\n",
      "554 | 550 | 0.73\n",
      "554 | 551 | 1.0\n",
      "554 | 553 | 0.7\n",
      "555 | 549 | 0.88\n",
      "555 | 552 | 0.88\n",
      "556 | 550 | 1.0\n",
      "556 | 551 | 0.74\n",
      "556 | 553 | 0.98\n",
      "556 | 554 | 0.75\n",
      "557 | 550 | 0.72\n",
      "557 | 551 | 1.0\n",
      "557 | 553 | 0.69\n",
      "557 | 554 | 1.0\n",
      "557 | 556 | 0.75\n",
      "560 | 558 | 0.63\n",
      "560 | 559 | 0.89\n",
      "561 | 559 | 0.98\n",
      "561 | 560 | 0.82\n",
      "566 | 564 | 0.98\n",
      "567 | 565 | 0.99\n",
      "568 | 564 | 1.0\n",
      "568 | 566 | 0.98\n",
      "569 | 565 | 0.94\n",
      "569 | 567 | 0.93\n",
      "570 | 72 | 0.52\n",
      "570 | 73 | 0.54\n",
      "573 | 572 | 0.79\n",
      "574 | 572 | 0.99\n",
      "574 | 573 | 0.78\n",
      "575 | 572 | 0.78\n",
      "575 | 573 | 0.98\n",
      "575 | 574 | 0.77\n",
      "576 | 572 | 0.99\n",
      "576 | 573 | 0.79\n",
      "576 | 574 | 0.99\n",
      "576 | 575 | 0.78\n",
      "577 | 572 | 0.86\n",
      "577 | 573 | 0.96\n",
      "577 | 574 | 0.85\n",
      "577 | 575 | 0.93\n",
      "577 | 576 | 0.86\n",
      "580 | 579 | 0.98\n",
      "584 | 583 | 0.99\n",
      "585 | 583 | 1.0\n",
      "585 | 584 | 1.0\n",
      "586 | 578 | 1.0\n",
      "587 | 579 | 1.0\n",
      "587 | 580 | 0.98\n",
      "588 | 579 | 0.98\n",
      "588 | 580 | 1.0\n",
      "588 | 587 | 0.97\n",
      "589 | 581 | 1.0\n"
     ]
    }
   ],
   "execution_count": 919
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T17:05:40.879017Z",
     "start_time": "2024-09-29T17:05:40.868648Z"
    }
   },
   "cell_type": "code",
   "source": "X_train",
   "id": "900bf479fb3bd608",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            0        1          2          3       4      5       8       9  \\\n",
       "425   3079.17  2405.56  2217.3777  1425.1041  1.7585  100.0  1.4794 -0.0198   \n",
       "412   2989.85  2501.88  2197.2333  1435.1460  0.9740  100.0  1.5330 -0.0059   \n",
       "115   3002.85  2502.05  2232.5889  1717.2750  1.6700  100.0  1.4518  0.0066   \n",
       "887   3007.75  2535.14  2216.5000  1111.5436  0.8373  100.0  1.4503  0.0149   \n",
       "328   2894.04  2490.06  2207.0444  1330.6718  1.3076  100.0  1.5546 -0.0107   \n",
       "...       ...      ...        ...        ...     ...    ...     ...     ...   \n",
       "1332  3045.48  2408.85  2223.0444  1194.5986  1.2016  100.0  1.3838 -0.0125   \n",
       "1447  2949.12  2553.24  2176.8000  1461.4374  0.8864  100.0  1.5576 -0.0205   \n",
       "1412  3025.46  2516.06  2195.9778  1388.2869  1.5605  100.0  1.4298  0.0122   \n",
       "270   2988.52  2291.92  2183.5777  1764.5386  1.7050  100.0  1.4305  0.0001   \n",
       "734   3069.73  2533.45  2218.6333   870.5620  1.3084  100.0  1.4645  0.0148   \n",
       "\n",
       "          10      11  ...     563   564     565     571      572     578  \\\n",
       "425  -0.0004  0.9535  ...     NaN   NaN     NaN  2.2846   9.3600  0.0234   \n",
       "412   0.0228  0.9490  ...  0.5802  6.11  0.1208  2.2688  11.1800     NaN   \n",
       "115   0.0151  0.9659  ...     NaN   NaN     NaN  2.2788  11.6000  0.0364   \n",
       "887   0.0010  0.9580  ...  0.5671  4.98  0.0877  2.3473  10.9300     NaN   \n",
       "328   0.0072  0.9600  ...     NaN   NaN     NaN  1.8070   8.9200     NaN   \n",
       "...      ...     ...  ...     ...   ...     ...     ...      ...     ...   \n",
       "1332  0.0073  0.9726  ...  0.7429  7.92  0.2796  2.0396   8.0900     NaN   \n",
       "1447  0.0095  0.9777  ...  0.5671  4.98  0.0877  2.2909   9.1299     NaN   \n",
       "1412  0.0176  0.9611  ...  0.5671  4.98  0.0877  2.2909   9.1299  0.0378   \n",
       "270  -0.0054  0.9615  ...     NaN   NaN     NaN  1.6889   9.8800  0.0274   \n",
       "734   0.0016  0.9669  ...  0.6847  3.26  0.2135  1.9829   7.0200     NaN   \n",
       "\n",
       "         579       581     582     583  \n",
       "425   0.0073   31.3771  0.5080  0.0139  \n",
       "412      NaN       NaN  0.4976  0.0184  \n",
       "115   0.0166   45.6835  0.4995  0.0093  \n",
       "887      NaN       NaN  0.4948  0.0099  \n",
       "328      NaN       NaN  0.4963  0.0208  \n",
       "...      ...       ...     ...     ...  \n",
       "1332     NaN       NaN  0.4990  0.0137  \n",
       "1447     NaN       NaN  0.5031  0.0111  \n",
       "1412  0.0651  171.9936  0.4985  0.0169  \n",
       "270   0.0142   51.9067  0.4999  0.0095  \n",
       "734      NaN       NaN  0.4929  0.0155  \n",
       "\n",
       "[1253 rows x 166 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>563</th>\n",
       "      <th>564</th>\n",
       "      <th>565</th>\n",
       "      <th>571</th>\n",
       "      <th>572</th>\n",
       "      <th>578</th>\n",
       "      <th>579</th>\n",
       "      <th>581</th>\n",
       "      <th>582</th>\n",
       "      <th>583</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>3079.17</td>\n",
       "      <td>2405.56</td>\n",
       "      <td>2217.3777</td>\n",
       "      <td>1425.1041</td>\n",
       "      <td>1.7585</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.4794</td>\n",
       "      <td>-0.0198</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>0.9535</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.2846</td>\n",
       "      <td>9.3600</td>\n",
       "      <td>0.0234</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>31.3771</td>\n",
       "      <td>0.5080</td>\n",
       "      <td>0.0139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>2989.85</td>\n",
       "      <td>2501.88</td>\n",
       "      <td>2197.2333</td>\n",
       "      <td>1435.1460</td>\n",
       "      <td>0.9740</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.5330</td>\n",
       "      <td>-0.0059</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.9490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5802</td>\n",
       "      <td>6.11</td>\n",
       "      <td>0.1208</td>\n",
       "      <td>2.2688</td>\n",
       "      <td>11.1800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4976</td>\n",
       "      <td>0.0184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>3002.85</td>\n",
       "      <td>2502.05</td>\n",
       "      <td>2232.5889</td>\n",
       "      <td>1717.2750</td>\n",
       "      <td>1.6700</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.4518</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>0.9659</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.2788</td>\n",
       "      <td>11.6000</td>\n",
       "      <td>0.0364</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>45.6835</td>\n",
       "      <td>0.4995</td>\n",
       "      <td>0.0093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>3007.75</td>\n",
       "      <td>2535.14</td>\n",
       "      <td>2216.5000</td>\n",
       "      <td>1111.5436</td>\n",
       "      <td>0.8373</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.4503</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.9580</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5671</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.0877</td>\n",
       "      <td>2.3473</td>\n",
       "      <td>10.9300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4948</td>\n",
       "      <td>0.0099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>2894.04</td>\n",
       "      <td>2490.06</td>\n",
       "      <td>2207.0444</td>\n",
       "      <td>1330.6718</td>\n",
       "      <td>1.3076</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.5546</td>\n",
       "      <td>-0.0107</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.9600</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.8070</td>\n",
       "      <td>8.9200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4963</td>\n",
       "      <td>0.0208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>3045.48</td>\n",
       "      <td>2408.85</td>\n",
       "      <td>2223.0444</td>\n",
       "      <td>1194.5986</td>\n",
       "      <td>1.2016</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.3838</td>\n",
       "      <td>-0.0125</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.9726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7429</td>\n",
       "      <td>7.92</td>\n",
       "      <td>0.2796</td>\n",
       "      <td>2.0396</td>\n",
       "      <td>8.0900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>0.0137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>2949.12</td>\n",
       "      <td>2553.24</td>\n",
       "      <td>2176.8000</td>\n",
       "      <td>1461.4374</td>\n",
       "      <td>0.8864</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.5576</td>\n",
       "      <td>-0.0205</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.9777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5671</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.0877</td>\n",
       "      <td>2.2909</td>\n",
       "      <td>9.1299</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5031</td>\n",
       "      <td>0.0111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1412</th>\n",
       "      <td>3025.46</td>\n",
       "      <td>2516.06</td>\n",
       "      <td>2195.9778</td>\n",
       "      <td>1388.2869</td>\n",
       "      <td>1.5605</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.4298</td>\n",
       "      <td>0.0122</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.9611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5671</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.0877</td>\n",
       "      <td>2.2909</td>\n",
       "      <td>9.1299</td>\n",
       "      <td>0.0378</td>\n",
       "      <td>0.0651</td>\n",
       "      <td>171.9936</td>\n",
       "      <td>0.4985</td>\n",
       "      <td>0.0169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>2988.52</td>\n",
       "      <td>2291.92</td>\n",
       "      <td>2183.5777</td>\n",
       "      <td>1764.5386</td>\n",
       "      <td>1.7050</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.4305</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.0054</td>\n",
       "      <td>0.9615</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.6889</td>\n",
       "      <td>9.8800</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>0.0142</td>\n",
       "      <td>51.9067</td>\n",
       "      <td>0.4999</td>\n",
       "      <td>0.0095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>3069.73</td>\n",
       "      <td>2533.45</td>\n",
       "      <td>2218.6333</td>\n",
       "      <td>870.5620</td>\n",
       "      <td>1.3084</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.4645</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.9669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6847</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.2135</td>\n",
       "      <td>1.9829</td>\n",
       "      <td>7.0200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4929</td>\n",
       "      <td>0.0155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1253 rows × 166 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 920,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 920
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create feature Handler\n",
    "### 1. Null 값을 핸들링하는데, 데이터의 대부분이 비어있으면 (대략 1/3) 해당 컬럼을 drop합니다.\n",
    "### 2. NaN value는 이전값 또는 이후 값으로 채웁니다. (이런 방식은 추천되지 않지만, 해당 데이터에서는 운이 좋게도 이런 방식으로 진행했을 때 데이터 무결성이 보장되는 것을 확인했습니다.)\n",
    "### 3. zero 값으로 이루어진 column은 drop합니다.\n",
    "### 4. smote를 통해 데이터 oversampling을 하는데 해당 알고리즘은 knn그룹 상에서 비슷한 데이터 분포를 가상으로 만들어내는 것으로 알고 있습니다.\n",
    "### 5. log scaler을 처음에 사용하려 했지만 NaN으로 값이 발산하는 문제가 있기에 std_scaler를 사용하겠습니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46f3a2f3eeca38a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T17:05:40.908914Z",
     "start_time": "2024-09-29T17:05:40.879729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def NullDropHandler(train, test):\n",
    "    tmp_stack = []\n",
    "    for column in train.columns:\n",
    "        if train[column].isnull().sum() > 500:\n",
    "            train = train.drop([column], axis=1)\n",
    "            test = test.drop([column], axis=1)\n",
    "            tmp_stack.append(column)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def NaNValueHandler(df: pd.DataFrame):\n",
    "    df = df.ffill()\n",
    "    df = df.bfill()\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_zero_columns(train, test):\n",
    "    zero_columns = train.columns[(train.mean() == 0) & (train.max() == 0) & (train.min() == 0)]\n",
    "    train = train.drop(zero_columns, axis=1)\n",
    "    test = test.drop(zero_columns, axis=1)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def augment_data_with_smote(X, y):\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "\n",
    "'''def LogTransform(train, test):\n",
    "    numeric_features = train.select_dtypes(include=['int64', 'float64']).columns\n",
    "    # Use np.where to handle negative values before applying log\n",
    "    train[numeric_features] = train[numeric_features].apply(lambda x: np.log1p(np.where(x < 0, 0, x)))\n",
    "    test[numeric_features] = test[numeric_features].apply(lambda x: np.log1p(np.where(x < 0, 0, x)))\n",
    "    return train, test'''\n",
    "\n",
    "\n",
    "def DataHandler(X_train, X_test, y_train, y_test):\n",
    "    X_train, X_test = NullDropHandler(X_train.copy(), X_test.copy())\n",
    "    X_train = NaNValueHandler(X_train)\n",
    "    X_test = NaNValueHandler(X_test)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    X_train, X_test = drop_zero_columns(pd.DataFrame(X_train), pd.DataFrame(X_test))\n",
    "    X_train, y_train = augment_data_with_smote(X_train, y_train)\n",
    "\n",
    "    return X_train, X_test, np.array(y_train).ravel(), np.array(y_test).ravel()\n",
    "\n",
    "\n",
    "# Usage\n",
    "X_train, X_test, y_train, y_test = DataHandler(X_train, X_test, y_train, y_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = pd.DataFrame(X_train), pd.DataFrame(X_test), pd.DataFrame(y_train), pd.DataFrame(\n",
    "    y_test)"
   ],
   "id": "9b9af20ebceef174",
   "outputs": [],
   "execution_count": 921
  },
  {
   "cell_type": "code",
   "source": [
    "print('is NaN data check : ', X_train.isna().sum().sum())\n",
    "print('is NaN data check : ', X_test.isna().sum().sum())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T17:05:40.916997Z",
     "start_time": "2024-09-29T17:05:40.911740Z"
    }
   },
   "id": "3eff3c4302655af4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is NaN data check :  0\n",
      "is NaN data check :  0\n"
     ]
    }
   ],
   "execution_count": 922
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T17:05:40.927544Z",
     "start_time": "2024-09-29T17:05:40.917981Z"
    }
   },
   "cell_type": "code",
   "source": "X_train",
   "id": "b9227350903d081f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           0         1         2         3         4         6         7    \\\n",
       "0     0.881309 -1.100675  0.568447  0.040984 -0.049872  0.209377 -1.238595   \n",
       "1    -0.325747  0.086066 -0.113297  0.062878 -0.062384  0.925505 -0.326870   \n",
       "2    -0.150067  0.088160  1.083238  0.677975 -0.051283 -0.159375  0.493027   \n",
       "3    -0.083850  0.495856  0.538743 -0.642640 -0.064564 -0.179416  1.037438   \n",
       "4    -1.620508 -0.059566  0.218739 -0.164897 -0.057063  1.214093 -0.641710   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2335 -1.098304 -0.006497  0.148130 -0.198164 -0.054654  0.081778  1.030151   \n",
       "2336  0.428660 -0.873879 -0.437387 -0.555109 -0.063702 -0.179439 -0.355186   \n",
       "2337  0.847122 -0.088783 -0.270408 -0.229245 -0.057170 -0.652952 -0.454887   \n",
       "2338 -1.118166 -0.328587 -0.403791  0.117403 -0.063614  0.557276 -0.938778   \n",
       "2339 -1.063457  0.339198 -0.155242  0.416675 -0.052482  0.325634 -0.396013   \n",
       "\n",
       "           8         9         10   ...       146       147       148  \\\n",
       "0    -0.079477 -1.136553 -0.937733  ... -0.720581 -0.884802  1.053761   \n",
       "1     2.429340 -1.608314 -1.002535  ...  0.146397  0.893408  1.053761   \n",
       "2     1.596672  0.163412 -0.415426  ... -0.538443 -0.777107  1.053761   \n",
       "3     0.071917 -0.664792 -0.773402  ... -0.376947 -0.885298  0.215580   \n",
       "4     0.742377 -0.455120  0.264198  ... -0.466802 -0.621768  0.215580   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2335 -0.746428 -0.258622  0.844106  ... -0.464426 -0.425466 -1.058568   \n",
       "2336  0.270431  0.758173 -0.301445  ... -0.431059  0.956964 -1.458157   \n",
       "2337 -0.270698  0.119768 -2.008651  ...  0.049044  0.511741  0.040958   \n",
       "2338  0.873380  0.506746 -0.590578  ... -0.498256 -0.601556 -2.002622   \n",
       "2339 -1.625094 -1.590122 -0.290053  ...  0.241388 -0.564872  0.524862   \n",
       "\n",
       "           149       150       151       152       153       154       155  \n",
       "0    -0.823185 -0.110345 -0.304470  0.657738 -0.226864  2.315336 -0.078402  \n",
       "1    -0.823185 -0.110345 -0.304470  0.601695 -0.206288 -0.714307  0.157986  \n",
       "2    -0.823185 -0.110345 -0.304470  0.637165 -0.201539 -0.160814 -0.320042  \n",
       "3    -0.930100 -0.545908 -0.722336  0.880136 -0.209114 -1.529980 -0.288524  \n",
       "4    -0.930100 -0.545908 -0.722336 -1.036322 -0.231838 -1.093012  0.284059  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "2335  1.073903  1.237163 -0.128304  1.048743 -0.252008  0.846804  0.022656  \n",
       "2336 -0.469792 -0.300080  0.488629  0.912379 -0.218612  0.835660 -0.057084  \n",
       "2337 -0.448858  0.011993 -0.870104  0.152377 -0.265283 -0.340871 -0.050825  \n",
       "2338 -0.159644 -0.939930  0.357896 -0.342407 -0.232328  1.220270 -0.164780  \n",
       "2339  0.441059 -1.089786  1.035096  1.370138  2.706003  0.116425  0.078979  \n",
       "\n",
       "[2340 rows x 152 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.881309</td>\n",
       "      <td>-1.100675</td>\n",
       "      <td>0.568447</td>\n",
       "      <td>0.040984</td>\n",
       "      <td>-0.049872</td>\n",
       "      <td>0.209377</td>\n",
       "      <td>-1.238595</td>\n",
       "      <td>-0.079477</td>\n",
       "      <td>-1.136553</td>\n",
       "      <td>-0.937733</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.720581</td>\n",
       "      <td>-0.884802</td>\n",
       "      <td>1.053761</td>\n",
       "      <td>-0.823185</td>\n",
       "      <td>-0.110345</td>\n",
       "      <td>-0.304470</td>\n",
       "      <td>0.657738</td>\n",
       "      <td>-0.226864</td>\n",
       "      <td>2.315336</td>\n",
       "      <td>-0.078402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.325747</td>\n",
       "      <td>0.086066</td>\n",
       "      <td>-0.113297</td>\n",
       "      <td>0.062878</td>\n",
       "      <td>-0.062384</td>\n",
       "      <td>0.925505</td>\n",
       "      <td>-0.326870</td>\n",
       "      <td>2.429340</td>\n",
       "      <td>-1.608314</td>\n",
       "      <td>-1.002535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146397</td>\n",
       "      <td>0.893408</td>\n",
       "      <td>1.053761</td>\n",
       "      <td>-0.823185</td>\n",
       "      <td>-0.110345</td>\n",
       "      <td>-0.304470</td>\n",
       "      <td>0.601695</td>\n",
       "      <td>-0.206288</td>\n",
       "      <td>-0.714307</td>\n",
       "      <td>0.157986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.150067</td>\n",
       "      <td>0.088160</td>\n",
       "      <td>1.083238</td>\n",
       "      <td>0.677975</td>\n",
       "      <td>-0.051283</td>\n",
       "      <td>-0.159375</td>\n",
       "      <td>0.493027</td>\n",
       "      <td>1.596672</td>\n",
       "      <td>0.163412</td>\n",
       "      <td>-0.415426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.538443</td>\n",
       "      <td>-0.777107</td>\n",
       "      <td>1.053761</td>\n",
       "      <td>-0.823185</td>\n",
       "      <td>-0.110345</td>\n",
       "      <td>-0.304470</td>\n",
       "      <td>0.637165</td>\n",
       "      <td>-0.201539</td>\n",
       "      <td>-0.160814</td>\n",
       "      <td>-0.320042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.083850</td>\n",
       "      <td>0.495856</td>\n",
       "      <td>0.538743</td>\n",
       "      <td>-0.642640</td>\n",
       "      <td>-0.064564</td>\n",
       "      <td>-0.179416</td>\n",
       "      <td>1.037438</td>\n",
       "      <td>0.071917</td>\n",
       "      <td>-0.664792</td>\n",
       "      <td>-0.773402</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.376947</td>\n",
       "      <td>-0.885298</td>\n",
       "      <td>0.215580</td>\n",
       "      <td>-0.930100</td>\n",
       "      <td>-0.545908</td>\n",
       "      <td>-0.722336</td>\n",
       "      <td>0.880136</td>\n",
       "      <td>-0.209114</td>\n",
       "      <td>-1.529980</td>\n",
       "      <td>-0.288524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.620508</td>\n",
       "      <td>-0.059566</td>\n",
       "      <td>0.218739</td>\n",
       "      <td>-0.164897</td>\n",
       "      <td>-0.057063</td>\n",
       "      <td>1.214093</td>\n",
       "      <td>-0.641710</td>\n",
       "      <td>0.742377</td>\n",
       "      <td>-0.455120</td>\n",
       "      <td>0.264198</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.466802</td>\n",
       "      <td>-0.621768</td>\n",
       "      <td>0.215580</td>\n",
       "      <td>-0.930100</td>\n",
       "      <td>-0.545908</td>\n",
       "      <td>-0.722336</td>\n",
       "      <td>-1.036322</td>\n",
       "      <td>-0.231838</td>\n",
       "      <td>-1.093012</td>\n",
       "      <td>0.284059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2335</th>\n",
       "      <td>-1.098304</td>\n",
       "      <td>-0.006497</td>\n",
       "      <td>0.148130</td>\n",
       "      <td>-0.198164</td>\n",
       "      <td>-0.054654</td>\n",
       "      <td>0.081778</td>\n",
       "      <td>1.030151</td>\n",
       "      <td>-0.746428</td>\n",
       "      <td>-0.258622</td>\n",
       "      <td>0.844106</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.464426</td>\n",
       "      <td>-0.425466</td>\n",
       "      <td>-1.058568</td>\n",
       "      <td>1.073903</td>\n",
       "      <td>1.237163</td>\n",
       "      <td>-0.128304</td>\n",
       "      <td>1.048743</td>\n",
       "      <td>-0.252008</td>\n",
       "      <td>0.846804</td>\n",
       "      <td>0.022656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2336</th>\n",
       "      <td>0.428660</td>\n",
       "      <td>-0.873879</td>\n",
       "      <td>-0.437387</td>\n",
       "      <td>-0.555109</td>\n",
       "      <td>-0.063702</td>\n",
       "      <td>-0.179439</td>\n",
       "      <td>-0.355186</td>\n",
       "      <td>0.270431</td>\n",
       "      <td>0.758173</td>\n",
       "      <td>-0.301445</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.431059</td>\n",
       "      <td>0.956964</td>\n",
       "      <td>-1.458157</td>\n",
       "      <td>-0.469792</td>\n",
       "      <td>-0.300080</td>\n",
       "      <td>0.488629</td>\n",
       "      <td>0.912379</td>\n",
       "      <td>-0.218612</td>\n",
       "      <td>0.835660</td>\n",
       "      <td>-0.057084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2337</th>\n",
       "      <td>0.847122</td>\n",
       "      <td>-0.088783</td>\n",
       "      <td>-0.270408</td>\n",
       "      <td>-0.229245</td>\n",
       "      <td>-0.057170</td>\n",
       "      <td>-0.652952</td>\n",
       "      <td>-0.454887</td>\n",
       "      <td>-0.270698</td>\n",
       "      <td>0.119768</td>\n",
       "      <td>-2.008651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049044</td>\n",
       "      <td>0.511741</td>\n",
       "      <td>0.040958</td>\n",
       "      <td>-0.448858</td>\n",
       "      <td>0.011993</td>\n",
       "      <td>-0.870104</td>\n",
       "      <td>0.152377</td>\n",
       "      <td>-0.265283</td>\n",
       "      <td>-0.340871</td>\n",
       "      <td>-0.050825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2338</th>\n",
       "      <td>-1.118166</td>\n",
       "      <td>-0.328587</td>\n",
       "      <td>-0.403791</td>\n",
       "      <td>0.117403</td>\n",
       "      <td>-0.063614</td>\n",
       "      <td>0.557276</td>\n",
       "      <td>-0.938778</td>\n",
       "      <td>0.873380</td>\n",
       "      <td>0.506746</td>\n",
       "      <td>-0.590578</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.498256</td>\n",
       "      <td>-0.601556</td>\n",
       "      <td>-2.002622</td>\n",
       "      <td>-0.159644</td>\n",
       "      <td>-0.939930</td>\n",
       "      <td>0.357896</td>\n",
       "      <td>-0.342407</td>\n",
       "      <td>-0.232328</td>\n",
       "      <td>1.220270</td>\n",
       "      <td>-0.164780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2339</th>\n",
       "      <td>-1.063457</td>\n",
       "      <td>0.339198</td>\n",
       "      <td>-0.155242</td>\n",
       "      <td>0.416675</td>\n",
       "      <td>-0.052482</td>\n",
       "      <td>0.325634</td>\n",
       "      <td>-0.396013</td>\n",
       "      <td>-1.625094</td>\n",
       "      <td>-1.590122</td>\n",
       "      <td>-0.290053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241388</td>\n",
       "      <td>-0.564872</td>\n",
       "      <td>0.524862</td>\n",
       "      <td>0.441059</td>\n",
       "      <td>-1.089786</td>\n",
       "      <td>1.035096</td>\n",
       "      <td>1.370138</td>\n",
       "      <td>2.706003</td>\n",
       "      <td>0.116425</td>\n",
       "      <td>0.078979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2340 rows × 152 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 923,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 923
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### target 값을 확인해서 category와 smote를 통한 데이터 증강을 확인합니다.",
   "id": "778fadd20b042c82"
  },
  {
   "cell_type": "code",
   "source": [
    "tmp = pd.DataFrame(y_train)\n",
    "tmp.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T17:05:40.936190Z",
     "start_time": "2024-09-29T17:05:40.928407Z"
    }
   },
   "id": "6f1bce95e87ef666",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0 \n",
       "-1    1170\n",
       " 1    1170\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 924,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 924
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 0으로 이루어진 column index를 확인합니다. \n",
    "### index가 없으니 zero data 무결성이 보장되었습니다."
   ],
   "id": "a7604cfbabe50fd9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T17:05:40.948483Z",
     "start_time": "2024-09-29T17:05:40.936810Z"
    }
   },
   "cell_type": "code",
   "source": "print(X_train.columns[(X_train.mean() == 0) & (X_train.max() == 0) & (X_train.min() == 0)])",
   "id": "3df846bc96fd37e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([], dtype='int64')\n"
     ]
    }
   ],
   "execution_count": 925
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### data scaler를 통한 노말라이즈 또한 잘 이루어진 것을 볼 수 있습니다.",
   "id": "686a92b82e640af5"
  },
  {
   "cell_type": "code",
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T17:05:40.958852Z",
     "start_time": "2024-09-29T17:05:40.949271Z"
    }
   },
   "id": "a6bf7a3a30749a98",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           0         1         2         3         4         6         7    \\\n",
       "0     0.881309 -1.100675  0.568447  0.040984 -0.049872  0.209377 -1.238595   \n",
       "1    -0.325747  0.086066 -0.113297  0.062878 -0.062384  0.925505 -0.326870   \n",
       "2    -0.150067  0.088160  1.083238  0.677975 -0.051283 -0.159375  0.493027   \n",
       "3    -0.083850  0.495856  0.538743 -0.642640 -0.064564 -0.179416  1.037438   \n",
       "4    -1.620508 -0.059566  0.218739 -0.164897 -0.057063  1.214093 -0.641710   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2335 -1.098304 -0.006497  0.148130 -0.198164 -0.054654  0.081778  1.030151   \n",
       "2336  0.428660 -0.873879 -0.437387 -0.555109 -0.063702 -0.179439 -0.355186   \n",
       "2337  0.847122 -0.088783 -0.270408 -0.229245 -0.057170 -0.652952 -0.454887   \n",
       "2338 -1.118166 -0.328587 -0.403791  0.117403 -0.063614  0.557276 -0.938778   \n",
       "2339 -1.063457  0.339198 -0.155242  0.416675 -0.052482  0.325634 -0.396013   \n",
       "\n",
       "           8         9         10   ...       146       147       148  \\\n",
       "0    -0.079477 -1.136553 -0.937733  ... -0.720581 -0.884802  1.053761   \n",
       "1     2.429340 -1.608314 -1.002535  ...  0.146397  0.893408  1.053761   \n",
       "2     1.596672  0.163412 -0.415426  ... -0.538443 -0.777107  1.053761   \n",
       "3     0.071917 -0.664792 -0.773402  ... -0.376947 -0.885298  0.215580   \n",
       "4     0.742377 -0.455120  0.264198  ... -0.466802 -0.621768  0.215580   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2335 -0.746428 -0.258622  0.844106  ... -0.464426 -0.425466 -1.058568   \n",
       "2336  0.270431  0.758173 -0.301445  ... -0.431059  0.956964 -1.458157   \n",
       "2337 -0.270698  0.119768 -2.008651  ...  0.049044  0.511741  0.040958   \n",
       "2338  0.873380  0.506746 -0.590578  ... -0.498256 -0.601556 -2.002622   \n",
       "2339 -1.625094 -1.590122 -0.290053  ...  0.241388 -0.564872  0.524862   \n",
       "\n",
       "           149       150       151       152       153       154       155  \n",
       "0    -0.823185 -0.110345 -0.304470  0.657738 -0.226864  2.315336 -0.078402  \n",
       "1    -0.823185 -0.110345 -0.304470  0.601695 -0.206288 -0.714307  0.157986  \n",
       "2    -0.823185 -0.110345 -0.304470  0.637165 -0.201539 -0.160814 -0.320042  \n",
       "3    -0.930100 -0.545908 -0.722336  0.880136 -0.209114 -1.529980 -0.288524  \n",
       "4    -0.930100 -0.545908 -0.722336 -1.036322 -0.231838 -1.093012  0.284059  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "2335  1.073903  1.237163 -0.128304  1.048743 -0.252008  0.846804  0.022656  \n",
       "2336 -0.469792 -0.300080  0.488629  0.912379 -0.218612  0.835660 -0.057084  \n",
       "2337 -0.448858  0.011993 -0.870104  0.152377 -0.265283 -0.340871 -0.050825  \n",
       "2338 -0.159644 -0.939930  0.357896 -0.342407 -0.232328  1.220270 -0.164780  \n",
       "2339  0.441059 -1.089786  1.035096  1.370138  2.706003  0.116425  0.078979  \n",
       "\n",
       "[2340 rows x 152 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.881309</td>\n",
       "      <td>-1.100675</td>\n",
       "      <td>0.568447</td>\n",
       "      <td>0.040984</td>\n",
       "      <td>-0.049872</td>\n",
       "      <td>0.209377</td>\n",
       "      <td>-1.238595</td>\n",
       "      <td>-0.079477</td>\n",
       "      <td>-1.136553</td>\n",
       "      <td>-0.937733</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.720581</td>\n",
       "      <td>-0.884802</td>\n",
       "      <td>1.053761</td>\n",
       "      <td>-0.823185</td>\n",
       "      <td>-0.110345</td>\n",
       "      <td>-0.304470</td>\n",
       "      <td>0.657738</td>\n",
       "      <td>-0.226864</td>\n",
       "      <td>2.315336</td>\n",
       "      <td>-0.078402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.325747</td>\n",
       "      <td>0.086066</td>\n",
       "      <td>-0.113297</td>\n",
       "      <td>0.062878</td>\n",
       "      <td>-0.062384</td>\n",
       "      <td>0.925505</td>\n",
       "      <td>-0.326870</td>\n",
       "      <td>2.429340</td>\n",
       "      <td>-1.608314</td>\n",
       "      <td>-1.002535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146397</td>\n",
       "      <td>0.893408</td>\n",
       "      <td>1.053761</td>\n",
       "      <td>-0.823185</td>\n",
       "      <td>-0.110345</td>\n",
       "      <td>-0.304470</td>\n",
       "      <td>0.601695</td>\n",
       "      <td>-0.206288</td>\n",
       "      <td>-0.714307</td>\n",
       "      <td>0.157986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.150067</td>\n",
       "      <td>0.088160</td>\n",
       "      <td>1.083238</td>\n",
       "      <td>0.677975</td>\n",
       "      <td>-0.051283</td>\n",
       "      <td>-0.159375</td>\n",
       "      <td>0.493027</td>\n",
       "      <td>1.596672</td>\n",
       "      <td>0.163412</td>\n",
       "      <td>-0.415426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.538443</td>\n",
       "      <td>-0.777107</td>\n",
       "      <td>1.053761</td>\n",
       "      <td>-0.823185</td>\n",
       "      <td>-0.110345</td>\n",
       "      <td>-0.304470</td>\n",
       "      <td>0.637165</td>\n",
       "      <td>-0.201539</td>\n",
       "      <td>-0.160814</td>\n",
       "      <td>-0.320042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.083850</td>\n",
       "      <td>0.495856</td>\n",
       "      <td>0.538743</td>\n",
       "      <td>-0.642640</td>\n",
       "      <td>-0.064564</td>\n",
       "      <td>-0.179416</td>\n",
       "      <td>1.037438</td>\n",
       "      <td>0.071917</td>\n",
       "      <td>-0.664792</td>\n",
       "      <td>-0.773402</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.376947</td>\n",
       "      <td>-0.885298</td>\n",
       "      <td>0.215580</td>\n",
       "      <td>-0.930100</td>\n",
       "      <td>-0.545908</td>\n",
       "      <td>-0.722336</td>\n",
       "      <td>0.880136</td>\n",
       "      <td>-0.209114</td>\n",
       "      <td>-1.529980</td>\n",
       "      <td>-0.288524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.620508</td>\n",
       "      <td>-0.059566</td>\n",
       "      <td>0.218739</td>\n",
       "      <td>-0.164897</td>\n",
       "      <td>-0.057063</td>\n",
       "      <td>1.214093</td>\n",
       "      <td>-0.641710</td>\n",
       "      <td>0.742377</td>\n",
       "      <td>-0.455120</td>\n",
       "      <td>0.264198</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.466802</td>\n",
       "      <td>-0.621768</td>\n",
       "      <td>0.215580</td>\n",
       "      <td>-0.930100</td>\n",
       "      <td>-0.545908</td>\n",
       "      <td>-0.722336</td>\n",
       "      <td>-1.036322</td>\n",
       "      <td>-0.231838</td>\n",
       "      <td>-1.093012</td>\n",
       "      <td>0.284059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2335</th>\n",
       "      <td>-1.098304</td>\n",
       "      <td>-0.006497</td>\n",
       "      <td>0.148130</td>\n",
       "      <td>-0.198164</td>\n",
       "      <td>-0.054654</td>\n",
       "      <td>0.081778</td>\n",
       "      <td>1.030151</td>\n",
       "      <td>-0.746428</td>\n",
       "      <td>-0.258622</td>\n",
       "      <td>0.844106</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.464426</td>\n",
       "      <td>-0.425466</td>\n",
       "      <td>-1.058568</td>\n",
       "      <td>1.073903</td>\n",
       "      <td>1.237163</td>\n",
       "      <td>-0.128304</td>\n",
       "      <td>1.048743</td>\n",
       "      <td>-0.252008</td>\n",
       "      <td>0.846804</td>\n",
       "      <td>0.022656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2336</th>\n",
       "      <td>0.428660</td>\n",
       "      <td>-0.873879</td>\n",
       "      <td>-0.437387</td>\n",
       "      <td>-0.555109</td>\n",
       "      <td>-0.063702</td>\n",
       "      <td>-0.179439</td>\n",
       "      <td>-0.355186</td>\n",
       "      <td>0.270431</td>\n",
       "      <td>0.758173</td>\n",
       "      <td>-0.301445</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.431059</td>\n",
       "      <td>0.956964</td>\n",
       "      <td>-1.458157</td>\n",
       "      <td>-0.469792</td>\n",
       "      <td>-0.300080</td>\n",
       "      <td>0.488629</td>\n",
       "      <td>0.912379</td>\n",
       "      <td>-0.218612</td>\n",
       "      <td>0.835660</td>\n",
       "      <td>-0.057084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2337</th>\n",
       "      <td>0.847122</td>\n",
       "      <td>-0.088783</td>\n",
       "      <td>-0.270408</td>\n",
       "      <td>-0.229245</td>\n",
       "      <td>-0.057170</td>\n",
       "      <td>-0.652952</td>\n",
       "      <td>-0.454887</td>\n",
       "      <td>-0.270698</td>\n",
       "      <td>0.119768</td>\n",
       "      <td>-2.008651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049044</td>\n",
       "      <td>0.511741</td>\n",
       "      <td>0.040958</td>\n",
       "      <td>-0.448858</td>\n",
       "      <td>0.011993</td>\n",
       "      <td>-0.870104</td>\n",
       "      <td>0.152377</td>\n",
       "      <td>-0.265283</td>\n",
       "      <td>-0.340871</td>\n",
       "      <td>-0.050825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2338</th>\n",
       "      <td>-1.118166</td>\n",
       "      <td>-0.328587</td>\n",
       "      <td>-0.403791</td>\n",
       "      <td>0.117403</td>\n",
       "      <td>-0.063614</td>\n",
       "      <td>0.557276</td>\n",
       "      <td>-0.938778</td>\n",
       "      <td>0.873380</td>\n",
       "      <td>0.506746</td>\n",
       "      <td>-0.590578</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.498256</td>\n",
       "      <td>-0.601556</td>\n",
       "      <td>-2.002622</td>\n",
       "      <td>-0.159644</td>\n",
       "      <td>-0.939930</td>\n",
       "      <td>0.357896</td>\n",
       "      <td>-0.342407</td>\n",
       "      <td>-0.232328</td>\n",
       "      <td>1.220270</td>\n",
       "      <td>-0.164780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2339</th>\n",
       "      <td>-1.063457</td>\n",
       "      <td>0.339198</td>\n",
       "      <td>-0.155242</td>\n",
       "      <td>0.416675</td>\n",
       "      <td>-0.052482</td>\n",
       "      <td>0.325634</td>\n",
       "      <td>-0.396013</td>\n",
       "      <td>-1.625094</td>\n",
       "      <td>-1.590122</td>\n",
       "      <td>-0.290053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241388</td>\n",
       "      <td>-0.564872</td>\n",
       "      <td>0.524862</td>\n",
       "      <td>0.441059</td>\n",
       "      <td>-1.089786</td>\n",
       "      <td>1.035096</td>\n",
       "      <td>1.370138</td>\n",
       "      <td>2.706003</td>\n",
       "      <td>0.116425</td>\n",
       "      <td>0.078979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2340 rows × 152 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 926,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 926
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### train데이터의 shape을 확인해 smote의 적용이 잘 되었는지 확인합니다. 천 개의 데이터에서 2340개의 데이터로 oversampling 되었다는 것을 볼 수 있습니다.",
   "id": "b0f995adef70de58"
  },
  {
   "cell_type": "code",
   "source": [
    "X_train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T17:05:40.961620Z",
     "start_time": "2024-09-29T17:05:40.959587Z"
    }
   },
   "id": "7c83a6e869035de1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2340, 152)"
      ]
     },
     "execution_count": 927,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 927
  },
  {
   "cell_type": "code",
   "source": [
    "y_train = y_train.values.ravel()\n",
    "y_test = y_test.values.ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T17:05:40.963727Z",
     "start_time": "2024-09-29T17:05:40.962108Z"
    }
   },
   "id": "7001dce5d43a5364",
   "outputs": [],
   "execution_count": 928
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 차원의 저주, high-d feature handling\n",
    "### 1. 처음에는 xgboost를 통해 xgboost가 loss를 떨어뜨리기 위해 어떤 feature를 중요하게 봤는지 확인한 후 해당 feature에서 200개의 feature만 sampling 하기로 했지만, 이는 잘 작동하지 않았습니다.\n",
    "### 2. 또한 feature selection에서 시간 지연을 방지하기 위해서 간단한 모델을 RFE를 통해 feature selection을 진행했습니다.\n",
    "### 3. 이후에 PCA 차원축소를 같이 진행했습니다. 축소 후 분산을 확인해서 해당 데이터가 원본 데이터를 얼마나 잘 대변하는가? 를 확인하여 0.95를 가이드라인으로 잡았습니다. 이는 150입니다."
   ],
   "id": "468c8ceefa41459f"
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "lr = LogisticRegression(random_state=1234)\n",
    "rfe = RFE(lr, n_features_to_select=300)\n",
    "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
    "X_test_rfe = rfe.transform(X_test)\n",
    "\n",
    "pca = PCA(n_components=150)\n",
    "X_train = pca.fit_transform(X_train_rfe)\n",
    "X_test = pca.transform(X_test_rfe)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T17:05:40.991940Z",
     "start_time": "2024-09-29T17:05:40.964381Z"
    }
   },
   "id": "c85ba9019a122a9d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/forml/lib/python3.10/site-packages/sklearn/feature_selection/_rfe.py:291: UserWarning: Found n_features_to_select=300 > n_features=152. There will be no feature selection and all features will be kept.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 929
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T17:05:40.995092Z",
     "start_time": "2024-09-29T17:05:40.992937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_train = np.where(y_train == -1, 0, 1)\n",
    "y_test = np.where(y_test == -1, 0, 1)"
   ],
   "id": "86195e3c360f02ed",
   "outputs": [],
   "execution_count": 930
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### -1, 1의 target에서 0, 1의 target으로 re-handling합니다.",
   "id": "19ea669e162c60a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T17:05:40.998810Z",
     "start_time": "2024-09-29T17:05:40.995680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tmp = pd.DataFrame(y_train)\n",
    "tmp.value_counts()"
   ],
   "id": "49c0e981afe37421",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "0    1170\n",
       "1    1170\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 931,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 931
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T17:05:41.378513Z",
     "start_time": "2024-09-29T17:05:40.999553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert the classes parameter to a numpy array\n",
    "\n",
    "y_train = pd.DataFrame(y_train)\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    min_child_weight=1,\n",
    "    n_estimators=100,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n"
   ],
   "id": "6d1fe90942c865ce",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=5, max_leaves=None,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)"
      ],
      "text/html": [
       "<style>#sk-container-id-24 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-24 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-24 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-24 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-24 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-24 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-24 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-24 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-24 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-24 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-24 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-24 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-24 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-24 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-24 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-24 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-24 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-24 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-24 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-24 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-24 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-24 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-24 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-24 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-24 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-24 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-24 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-24 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-24 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-24 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-24 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-24 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-24 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-24 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-24 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-24 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-24 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-24 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-24 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-24 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-24 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-24 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-24\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=5, max_leaves=None,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" checked><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;XGBClassifier<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=5, max_leaves=None,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre></div> </div></div></div></div>"
      ]
     },
     "execution_count": 932,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 932
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### cross validation을 통해서 train data가 모델에 적합한지 f1 score를 통해 확인합니다.",
   "id": "9855d10ace08975"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T17:05:43.125036Z",
     "start_time": "2024-09-29T17:05:41.379499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1')\n",
    "print(f'Cross-validated F1 scores: {scores}')"
   ],
   "id": "e2ece7a85332d9ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated F1 scores: [0.98920086 1.         0.99785867 1.         1.        ]\n"
     ]
    }
   ],
   "execution_count": 933
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### overfitting 방지를 위해 파라미터 수정을 조금 진행합니다.",
   "id": "b2440a3cb522a408"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T17:05:45.070377Z",
     "start_time": "2024-09-29T17:05:43.126314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "# KNeighborsClassifier를 위한 베이지안 최적화 설정\n",
    "search_space = {\n",
    "    'n_neighbors': Integer(1, 50),  # 이웃 수 (1 ~ 50 사이)\n",
    "    'weights': Categorical(['uniform', 'distance']),  # 가중치 옵션\n",
    "    'algorithm': Categorical(['auto', 'brute']),  # 알고리즘 선택\n",
    "    'leaf_size': Integer(10, 100),  # 리프 사이즈 (10 ~ 100 사이)\n",
    "    'p': Integer(1, 5), # Minkowski 거리에서 p 값 (1은 Manhattan 거리, 2는 Euclidean 거리)\n",
    "    'metric': Categorical(['minkowski', 'hamming', 'dice', 'russellrao', 'rogerstanimoto', \n",
    "                           'l2', 'yule', 'cosine', 'euclidean', 'sokalmichener'])\n",
    "}\n",
    "\n",
    "# KNeighborsClassifier 초기화\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# BayesSearchCV 설정\n",
    "opt = BayesSearchCV(\n",
    "    model,\n",
    "    search_space,\n",
    "    n_iter=100,  # 최대 30번의 파라미터 조합을 테스트\n",
    "    cv=5,  # 5-Fold Cross Validation\n",
    "    scoring='balanced_accuracy',  # balanced accuracy score를 기준으로 최적화\n",
    "    n_jobs=-1,  # 모든 CPU 코어 사용\n",
    "    random_state=123  # 결과 재현성을 위한 random_state\n",
    ")\n",
    "\n",
    "# 모델 최적화 (여기서 X_train, y_train 사용)\n",
    "opt.fit(X_train, y_train.values.squeeze(axis=1))\n",
    "\n",
    "# 최적의 파라미터 출력\n",
    "print(f\"Best Parameters: {opt.best_params_}\")\n",
    "\n",
    "# 최적 파라미터를 사용하여 다시 모델을 학습 (전체 훈련 데이터를 사용)\n",
    "best_model = opt.best_estimator_\n",
    "\n",
    "# 최적 모델을 이용한 테스트셋 예측\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# 테스트셋에서의 balanced accuracy 계산\n",
    "balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "print(f\"Balanced Accuracy on Test Set: {balanced_accuracy}\")\n"
   ],
   "id": "daeaeb323e4c5b9b",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[934], line 33\u001B[0m\n\u001B[1;32m     22\u001B[0m opt \u001B[38;5;241m=\u001B[39m BayesSearchCV(\n\u001B[1;32m     23\u001B[0m     model,\n\u001B[1;32m     24\u001B[0m     search_space,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     29\u001B[0m     random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m123\u001B[39m  \u001B[38;5;66;03m# 결과 재현성을 위한 random_state\u001B[39;00m\n\u001B[1;32m     30\u001B[0m )\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# 모델 최적화 (여기서 X_train, y_train 사용)\u001B[39;00m\n\u001B[0;32m---> 33\u001B[0m \u001B[43mopt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# 최적의 파라미터 출력\u001B[39;00m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest Parameters: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mopt\u001B[38;5;241m.\u001B[39mbest_params_\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/forml/lib/python3.10/site-packages/skopt/searchcv.py:542\u001B[0m, in \u001B[0;36mBayesSearchCV.fit\u001B[0;34m(self, X, y, groups, callback, **fit_params)\u001B[0m\n\u001B[1;32m    535\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrefit):\n\u001B[1;32m    536\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    537\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBayesSearchCV doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt support a callable refit, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    538\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt define an implicit score to \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    539\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moptimize\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    540\u001B[0m     )\n\u001B[0;32m--> 542\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgroups\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    544\u001B[0m \u001B[38;5;66;03m# BaseSearchCV never ranked train scores,\u001B[39;00m\n\u001B[1;32m    545\u001B[0m \u001B[38;5;66;03m# but apparently we used to ship this (back-compat)\u001B[39;00m\n\u001B[1;32m    546\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_train_score:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/forml/lib/python3.10/site-packages/sklearn/base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1471\u001B[0m     )\n\u001B[1;32m   1472\u001B[0m ):\n\u001B[0;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/forml/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1018\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[0;34m(self, X, y, **params)\u001B[0m\n\u001B[1;32m   1012\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[1;32m   1013\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[1;32m   1014\u001B[0m     )\n\u001B[1;32m   1016\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[0;32m-> 1018\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1020\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[1;32m   1021\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[1;32m   1022\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m/opt/anaconda3/envs/forml/lib/python3.10/site-packages/skopt/searchcv.py:599\u001B[0m, in \u001B[0;36mBayesSearchCV._run_search\u001B[0;34m(self, evaluate_candidates)\u001B[0m\n\u001B[1;32m    595\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m n_iter \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    596\u001B[0m     \u001B[38;5;66;03m# when n_iter < n_points points left for evaluation\u001B[39;00m\n\u001B[1;32m    597\u001B[0m     n_points_adjusted \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m(n_iter, n_points)\n\u001B[0;32m--> 599\u001B[0m     optim_result, score_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_step\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    600\u001B[0m \u001B[43m        \u001B[49m\u001B[43msearch_space\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    601\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    602\u001B[0m \u001B[43m        \u001B[49m\u001B[43mscore_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    603\u001B[0m \u001B[43m        \u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    604\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_points\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_points_adjusted\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    605\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    606\u001B[0m     n_iter \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m n_points\n\u001B[1;32m    608\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m eval_callbacks(callbacks, optim_result):\n",
      "File \u001B[0;32m/opt/anaconda3/envs/forml/lib/python3.10/site-packages/skopt/searchcv.py:453\u001B[0m, in \u001B[0;36mBayesSearchCV._step\u001B[0;34m(self, search_space, optimizer, score_name, evaluate_candidates, n_points)\u001B[0m\n\u001B[1;32m    450\u001B[0m \u001B[38;5;66;03m# make lists into dictionaries\u001B[39;00m\n\u001B[1;32m    451\u001B[0m params_dict \u001B[38;5;241m=\u001B[39m [point_asdict(search_space, p) \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m params]\n\u001B[0;32m--> 453\u001B[0m all_results \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams_dict\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    455\u001B[0m \u001B[38;5;66;03m# if self.scoring is a callable, we have to wait until here\u001B[39;00m\n\u001B[1;32m    456\u001B[0m \u001B[38;5;66;03m# to get the score name\u001B[39;00m\n\u001B[1;32m    457\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m score_name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/forml/lib/python3.10/site-packages/sklearn/model_selection/_search.py:964\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[0;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[1;32m    956\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    957\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[1;32m    958\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m candidates,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    959\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m fits\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    960\u001B[0m             n_splits, n_candidates, n_candidates \u001B[38;5;241m*\u001B[39m n_splits\n\u001B[1;32m    961\u001B[0m         )\n\u001B[1;32m    962\u001B[0m     )\n\u001B[0;32m--> 964\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    965\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    966\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    967\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    968\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    969\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    970\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    971\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    972\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    973\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcandidate_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_candidates\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    974\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_and_score_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    975\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    976\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    977\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    978\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplitter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    979\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    980\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    982\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    983\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    984\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo fits were performed. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    985\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWas the CV iterator empty? \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    986\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWere there no candidates?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    987\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/forml/lib/python3.10/site-packages/sklearn/utils/parallel.py:74\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m     69\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[1;32m     70\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     71\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[1;32m     73\u001B[0m )\n\u001B[0;32m---> 74\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/forml/lib/python3.10/site-packages/joblib/parallel.py:2007\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   2001\u001B[0m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[1;32m   2002\u001B[0m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[1;32m   2003\u001B[0m \u001B[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001B[39;00m\n\u001B[1;32m   2004\u001B[0m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[1;32m   2005\u001B[0m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[0;32m-> 2007\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/forml/lib/python3.10/site-packages/joblib/parallel.py:1650\u001B[0m, in \u001B[0;36mParallel._get_outputs\u001B[0;34m(self, iterator, pre_dispatch)\u001B[0m\n\u001B[1;32m   1647\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[1;32m   1649\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[0;32m-> 1650\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrieve()\n\u001B[1;32m   1652\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[1;32m   1653\u001B[0m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[1;32m   1654\u001B[0m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[1;32m   1655\u001B[0m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[1;32m   1656\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/forml/lib/python3.10/site-packages/joblib/parallel.py:1762\u001B[0m, in \u001B[0;36mParallel._retrieve\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# async callbacks to progress.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ((\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[1;32m   1760\u001B[0m     (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mget_status(\n\u001B[1;32m   1761\u001B[0m         timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout) \u001B[38;5;241m==\u001B[39m TASK_PENDING)):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1763\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m \u001B[38;5;66;03m# We need to be careful: the job list can be filling up as\u001B[39;00m\n\u001B[1;32m   1766\u001B[0m \u001B[38;5;66;03m# we empty it and Python list are not thread-safe by\u001B[39;00m\n\u001B[1;32m   1767\u001B[0m \u001B[38;5;66;03m# default hence the use of the lock\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 934
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T17:05:45.071579Z",
     "start_time": "2024-09-29T17:05:45.071400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "# SVC를 위한 베이지안 최적화 설정\n",
    "search_space = {\n",
    "    'C': Real(1e-6, 1000.0, prior='log-uniform'),  # 정규화 매개변수\n",
    "    'gamma': Real(1e-6, 1.0, prior='log-uniform'),  # 커널 계수\n",
    "    'kernel': Categorical(['linear', 'poly', 'rbf', 'sigmoid']),  # 커널 종류\n",
    "    'degree': Integer(1, 5),  # 다항 커널을 사용할 경우 차수\n",
    "    'class_weight': Categorical([None, 'balanced'])  # 클래스 가중치\n",
    "}\n",
    "\n",
    "# SVC 모델 초기화\n",
    "model = SVC()\n",
    "\n",
    "# BayesSearchCV 설정\n",
    "\n",
    "opt = BayesSearchCV(\n",
    "    model,\n",
    "    search_space,\n",
    "    n_iter=100,  # 최대 100번의 파라미터 조합을 테스트\n",
    "    cv=2,  # 5-Fold Cross Validation\n",
    "    scoring='balanced_accuracy',  # balanced accuracy score를 기준으로 최적화\n",
    "    n_jobs=-1,  # 모든 CPU 코어 사용\n",
    "    random_state=123  # 결과 재현성을 위한 random_state\n",
    ")\n",
    "\n",
    "# 모델 최적화 (여기서 X_train, y_train 사용)\n",
    "opt.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 파라미터 출력\n",
    "print(f\"Best Parameters: {opt.best_params_}\")\n",
    "\n",
    "# 최적 파라미터를 사용하여 다시 모델을 학습 (전체 훈련 데이터를 사용)\n",
    "best_model = opt.best_estimator_\n",
    "\n",
    "# 최적 모델을 이용한 테스트셋 예측\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# 테스트셋에서의 balanced accuracy 계산\n",
    "balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "print(f\"Balanced Accuracy on Test Set: {balanced_accuracy}\")\n"
   ],
   "id": "bf5d9e1b2f96d151",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T17:05:47.195006Z",
     "start_time": "2024-09-29T17:05:47.184067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = KNeighborsClassifier(n_neighbors=2,metric='cosine',algorithm='brute',weights='uniform')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "threshold = 0.4\n",
    "y_pred = (y_pred_proba >= threshold).astype(int)"
   ],
   "id": "2abf7cb5d1970d68",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/forml/lib/python3.10/site-packages/sklearn/neighbors/_classification.py:238: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    }
   ],
   "execution_count": 935
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "86426ddb6903b400"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T17:05:48.404825Z",
     "start_time": "2024-09-29T17:05:48.398149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "print(balanced_accuracy_score(y_test, y_pred))\n"
   ],
   "id": "e786d96d057243bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7038030229156509\n"
     ]
    }
   ],
   "execution_count": 936
  },
  {
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model, 'best_xgboost_model.pkl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b4c7f1d87efbaba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## conclusion\n",
    "\n",
    "### 1. 데이터의 분산이 크다보니 pass / fail 이 각각 특징적으로 갖는 feature 분포가 없습니다.\n",
    "### 2. 해당 데이터 column이 어떤 특성을 갖는지 분석을 진행할 수 없어 feature selection에 한계가 있습니다.\n",
    "### 3. 1번과 2번의 영향으로 f1스코어가 낮게 나옵니다. 또한 test데이터는 smote를 진행하지않아 이런 현상이 가중화됩니다.\n",
    "\n",
    "### 이상입니다."
   ],
   "id": "3ac66c3621eb2ec6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "#Rmx",
   "id": "af7b8ac91d5a39a4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
