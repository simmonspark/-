{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tqdm import tqdm\n",
    "import tarfile\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print('-------------------')\n",
    "print('|     lab1         |')\n",
    "print('-------------------')\n",
    "\n",
    "# 데이터 다운로드 및 추출\n",
    "url = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.tgz\"\n",
    "urllib.request.urlretrieve(url, \"housing.tgz\")\n",
    "\n",
    "tar = tarfile.open(\"housing.tgz\")\n",
    "tar.extractall()\n",
    "tar.close()\n",
    "\n",
    "# 데이터 로드\n",
    "housing = pd.read_csv(\"housing.csv\")\n",
    "\n",
    "print(housing.describe())\n",
    "print(housing.info())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-13T08:03:03.500882Z",
     "start_time": "2024-09-13T08:03:02.548606Z"
    }
   },
   "id": "dd249bfe19f7496b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "|     lab1         |\n",
      "-------------------\n",
      "          longitude      latitude  housing_median_age   total_rooms  \\\n",
      "count  20640.000000  20640.000000        20640.000000  20640.000000   \n",
      "mean    -119.569704     35.631861           28.639486   2635.763081   \n",
      "std        2.003532      2.135952           12.585558   2181.615252   \n",
      "min     -124.350000     32.540000            1.000000      2.000000   \n",
      "25%     -121.800000     33.930000           18.000000   1447.750000   \n",
      "50%     -118.490000     34.260000           29.000000   2127.000000   \n",
      "75%     -118.010000     37.710000           37.000000   3148.000000   \n",
      "max     -114.310000     41.950000           52.000000  39320.000000   \n",
      "\n",
      "       total_bedrooms    population    households  median_income  \\\n",
      "count    20433.000000  20640.000000  20640.000000   20640.000000   \n",
      "mean       537.870553   1425.476744    499.539680       3.870671   \n",
      "std        421.385070   1132.462122    382.329753       1.899822   \n",
      "min          1.000000      3.000000      1.000000       0.499900   \n",
      "25%        296.000000    787.000000    280.000000       2.563400   \n",
      "50%        435.000000   1166.000000    409.000000       3.534800   \n",
      "75%        647.000000   1725.000000    605.000000       4.743250   \n",
      "max       6445.000000  35682.000000   6082.000000      15.000100   \n",
      "\n",
      "       median_house_value  \n",
      "count        20640.000000  \n",
      "mean        206855.816909  \n",
      "std         115395.615874  \n",
      "min          14999.000000  \n",
      "25%         119600.000000  \n",
      "50%         179700.000000  \n",
      "75%         264725.000000  \n",
      "max         500001.000000  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   longitude           20640 non-null  float64\n",
      " 1   latitude            20640 non-null  float64\n",
      " 2   housing_median_age  20640 non-null  float64\n",
      " 3   total_rooms         20640 non-null  float64\n",
      " 4   total_bedrooms      20433 non-null  float64\n",
      " 5   population          20640 non-null  float64\n",
      " 6   households          20640 non-null  float64\n",
      " 7   median_income       20640 non-null  float64\n",
      " 8   median_house_value  20640 non-null  float64\n",
      " 9   ocean_proximity     20640 non-null  object \n",
      "dtypes: float64(9), object(1)\n",
      "memory usage: 1.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "housing\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e6e68dc0adedb4a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 이상치 탐지\n",
    "\n",
    "1. iqr 값을 계산해서 각 이상치 값이 있는 데이터의 인덱스를 찾아낸다.\n",
    "2. 인덱스의 중복을 set으로 제거한 후, 데이터에서 인덱스를 제거한다.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "921066d80fc9a2cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T08:03:03.514538Z",
     "start_time": "2024-09-13T08:03:03.510754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def AnomalyDelete(df: pd.DataFrame):\n",
    "    col = df.columns[:-1]\n",
    "    stack = []\n",
    "\n",
    "    quantile_25 = df[col].quantile(0.25)\n",
    "    quantile_75 = df[col].quantile(0.75)\n",
    "    iqr = quantile_75 - quantile_25\n",
    "    iqr_weight = iqr * 1.5\n",
    "\n",
    "    for i in col:\n",
    "        lowest_val = quantile_25[i] - iqr_weight[i]\n",
    "        highest_val = quantile_75[i] + iqr_weight[i]\n",
    "        outlier_idx = df[(df[i] < lowest_val) | (df[i] > highest_val)].index\n",
    "        stack.extend(outlier_idx)\n",
    "\n",
    "    return list(set(stack))\n",
    "\n",
    "\n",
    "def delHandler(X_train: pd.DataFrame, y_train: pd.Series, idxs):\n",
    "    X_train = X_train.drop(index=idxs, axis=0)\n",
    "    y_train = y_train.drop(index=idxs, axis=0)\n",
    "    return X_train, y_train\n",
    "\n",
    "\n",
    "def AnomalyHandler(X_train: pd.DataFrame, y_train):\n",
    "    idxs = AnomalyDelete(X_train)\n",
    "    X_train, y_train = delHandler(X_train, y_train, idxs)\n",
    "    return X_train, y_train"
   ],
   "id": "c88a20ac1d63c846",
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 전처리\n",
    "1. label 인코딩을 진행한다.\n",
    "2. 스탠다드 스케일링을 진행한다.\n",
    "3. train_df의 경우에는 fit_transform, test데이터는 transform을 진행한다. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "874efeaa79745c14"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T08:03:03.519310Z",
     "start_time": "2024-09-13T08:03:03.515815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CombinedAttributesAdder():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.le = LabelEncoder()\n",
    "        self.sk = StandardScaler()\n",
    "\n",
    "    def fit_and_transform(self, train_df, test_df):\n",
    "        train_df, test_df = self._DataHandler(train_df, test_df)\n",
    "        return train_df, test_df\n",
    "\n",
    "    def _Encoder(self, train_df, test_df):\n",
    "        feature = 'ocean_proximity'\n",
    "        train_df[feature] = self.le.fit_transform(train_df[feature])\n",
    "        test_df[feature] = self.le.transform(test_df[feature])\n",
    "        return train_df, test_df\n",
    "\n",
    "    def _Norm(self, train_df, test_df):\n",
    "        numeric_features = ['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
    "                            'total_bedrooms', 'population', 'households']\n",
    "        for feature in numeric_features:\n",
    "            train_df[feature] = self.sk.fit_transform(train_df[[feature]])\n",
    "            test_df[feature] = self.sk.transform(test_df[[feature]])\n",
    "        return train_df, test_df\n",
    "\n",
    "    def _DataHandler(self, train_df: pd.DataFrame, test_df: pd.DataFrame):\n",
    "        train_df, test_df = self._Encoder(train_df, test_df)\n",
    "        train_df, test_df = self._Norm(train_df, test_df)\n",
    "        return train_df, test_df"
   ],
   "id": "78a5226fa6707ab3",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T08:03:03.540720Z",
     "start_time": "2024-09-13T08:03:03.519900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "housing = housing.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_df = housing.iloc[:18000].copy()\n",
    "test_df = housing.iloc[18000:].copy()\n",
    "\n",
    "handler = CombinedAttributesAdder()\n",
    "train_df, test_df = handler.fit_and_transform(train_df=train_df, test_df=test_df)\n",
    "\n",
    "print(train_df.head())\n",
    "print(test_df.head())\n"
   ],
   "id": "3b7d47d490b350c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
      "0   0.585728 -0.812165            0.105369    -0.044086       -0.172520   \n",
      "1   0.700554 -0.755916            0.900971    -0.492269             NaN   \n",
      "2  -0.123203  0.575301           -0.928915    -0.131893       -0.396168   \n",
      "3   0.276194 -0.151243           -1.724517     2.729159        2.523153   \n",
      "4   1.169847 -1.351214            1.855694    -0.625809       -0.795880   \n",
      "\n",
      "   population  households  median_income  median_house_value  ocean_proximity  \n",
      "0   -0.322208   -0.208053         6.3434            460400.0                0  \n",
      "1    0.432096   -0.187142         2.2596            137500.0                0  \n",
      "2   -0.219070   -0.268170         4.2304             94400.0                1  \n",
      "3    3.031516    2.669763         3.9054             80900.0                1  \n",
      "4   -0.760107   -0.756955         7.2758            345200.0                4  \n",
      "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
      "18000  -0.238030  0.626862           -0.451553    -0.229762       -0.072592   \n",
      "18001   1.129907 -1.107470           -0.451553    -0.434645       -0.622196   \n",
      "18002   1.229756 -0.493422           -1.804077     1.134909        1.207436   \n",
      "18003  -0.956946  1.400281           -0.849354    -0.511019       -0.527026   \n",
      "18004  -1.276464  0.978416            0.741851    -0.669712       -0.600783   \n",
      "\n",
      "       population  households  median_income  median_house_value  \\\n",
      "18000    0.582432   -0.019857         1.3713             51800.0   \n",
      "18001   -0.172746   -0.613195         4.1713            133800.0   \n",
      "18002    0.448703    0.589163         3.1917            112800.0   \n",
      "18003   -0.316964   -0.508642         2.3438             89400.0   \n",
      "18004   -0.692805   -0.547849         3.0345            152100.0   \n",
      "\n",
      "       ocean_proximity  \n",
      "18000                1  \n",
      "18001                0  \n",
      "18002                1  \n",
      "18003                1  \n",
      "18004                3  \n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T08:03:03.558814Z",
     "start_time": "2024-09-13T08:03:03.541786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_x = train_df.drop(['median_house_value'], axis=1)\n",
    "test_x = test_df.drop(['median_house_value'], axis=1)\n",
    "\n",
    "train_y = train_df['median_house_value']\n",
    "test_y = test_df['median_house_value']\n",
    "\n",
    "train_x, train_y = AnomalyHandler(train_x, train_y)\n",
    "\n",
    "train_x[\"income_cat\"] = pd.cut(train_df[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])\n",
    "test_x[\"income_cat\"] = pd.cut(test_df[\"median_income\"],\n",
    "                              bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                              labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "train_x = pd.get_dummies(train_x, columns=['income_cat'])\n",
    "test_x = pd.get_dummies(test_x, columns=['income_cat'])"
   ],
   "id": "cc787c307566a939",
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GridSearchCV를 이용한 하이퍼파라미터 최적화"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee1e9d01c57a05a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T08:03:47.975447Z",
     "start_time": "2024-09-13T08:03:03.560117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "xgb_reg = xgb.XGBRegressor(random_state=123)\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'max_depth': [5, 7, 10],\n",
    "}\n",
    "grid_search = GridSearchCV(xgb_reg, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2)\n",
    "grid_search.fit(train_x, train_y)\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "test_predictions = best_model.predict(test_x)\n",
    "test_mse = mean_squared_error(test_y, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(test_y, test_predictions)\n",
    "\n",
    "print(\"테스트 세트 성능:\")\n",
    "print(\"RMSE:\", test_rmse)\n",
    "print(\"R2:\", test_r2)"
   ],
   "id": "83343e47adfed5b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "[CV] END ...learning_rate=0.01, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.01, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.01, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.01, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.01, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.01, max_depth=7, n_estimators=50; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.01, max_depth=7, n_estimators=50; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.01, max_depth=7, n_estimators=50; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.01, max_depth=7, n_estimators=50; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.01, max_depth=7, n_estimators=50; total time=   0.1s\n",
      "[CV] END ..learning_rate=0.01, max_depth=7, n_estimators=100; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.01, max_depth=7, n_estimators=100; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.01, max_depth=7, n_estimators=100; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.01, max_depth=7, n_estimators=100; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.01, max_depth=7, n_estimators=100; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.01, max_depth=7, n_estimators=200; total time=   0.5s\n",
      "[CV] END ..learning_rate=0.01, max_depth=7, n_estimators=200; total time=   0.6s\n",
      "[CV] END ..learning_rate=0.01, max_depth=7, n_estimators=200; total time=   0.5s\n",
      "[CV] END ..learning_rate=0.01, max_depth=7, n_estimators=200; total time=   0.5s\n",
      "[CV] END ..learning_rate=0.01, max_depth=7, n_estimators=200; total time=   0.4s\n",
      "[CV] END ..learning_rate=0.01, max_depth=10, n_estimators=50; total time=   0.4s\n",
      "[CV] END ..learning_rate=0.01, max_depth=10, n_estimators=50; total time=   0.3s\n",
      "[CV] END ..learning_rate=0.01, max_depth=10, n_estimators=50; total time=   0.4s\n",
      "[CV] END ..learning_rate=0.01, max_depth=10, n_estimators=50; total time=   0.4s\n",
      "[CV] END ..learning_rate=0.01, max_depth=10, n_estimators=50; total time=   0.3s\n",
      "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=100; total time=   0.7s\n",
      "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=100; total time=   0.7s\n",
      "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=100; total time=   0.7s\n",
      "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=100; total time=   0.7s\n",
      "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=100; total time=   0.7s\n",
      "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=200; total time=   1.4s\n",
      "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=200; total time=   1.3s\n",
      "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=200; total time=   1.2s\n",
      "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=200; total time=   1.2s\n",
      "[CV] END .learning_rate=0.01, max_depth=10, n_estimators=200; total time=   1.2s\n",
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END ....learning_rate=0.1, max_depth=7, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.1, max_depth=7, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.1, max_depth=7, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.1, max_depth=7, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.1, max_depth=7, n_estimators=50; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=200; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=200; total time=   0.4s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=200; total time=   0.4s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=200; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=200; total time=   0.4s\n",
      "[CV] END ...learning_rate=0.1, max_depth=10, n_estimators=50; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.1, max_depth=10, n_estimators=50; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.1, max_depth=10, n_estimators=50; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.1, max_depth=10, n_estimators=50; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.1, max_depth=10, n_estimators=50; total time=   0.3s\n",
      "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=100; total time=   0.5s\n",
      "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=100; total time=   0.5s\n",
      "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=100; total time=   0.5s\n",
      "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=100; total time=   0.5s\n",
      "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=100; total time=   0.5s\n",
      "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=200; total time=   0.9s\n",
      "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=200; total time=   0.9s\n",
      "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=200; total time=   0.8s\n",
      "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=200; total time=   0.9s\n",
      "[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=200; total time=   0.9s\n",
      "[CV] END ....learning_rate=0.3, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.3, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.3, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.3, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.3, max_depth=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.3, max_depth=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.3, max_depth=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.3, max_depth=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.3, max_depth=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.3, max_depth=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.3, max_depth=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.3, max_depth=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.3, max_depth=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.3, max_depth=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.3, max_depth=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END ....learning_rate=0.3, max_depth=7, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.3, max_depth=7, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.3, max_depth=7, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.3, max_depth=7, n_estimators=50; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.3, max_depth=7, n_estimators=50; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.3, max_depth=7, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.3, max_depth=7, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.3, max_depth=7, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.3, max_depth=7, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.3, max_depth=7, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.3, max_depth=7, n_estimators=200; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.3, max_depth=7, n_estimators=200; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.3, max_depth=7, n_estimators=200; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.3, max_depth=7, n_estimators=200; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.3, max_depth=7, n_estimators=200; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.3, max_depth=10, n_estimators=50; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.3, max_depth=10, n_estimators=50; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.3, max_depth=10, n_estimators=50; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.3, max_depth=10, n_estimators=50; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.3, max_depth=10, n_estimators=50; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.3, max_depth=10, n_estimators=100; total time=   0.5s\n",
      "[CV] END ..learning_rate=0.3, max_depth=10, n_estimators=100; total time=   0.4s\n",
      "[CV] END ..learning_rate=0.3, max_depth=10, n_estimators=100; total time=   0.4s\n",
      "[CV] END ..learning_rate=0.3, max_depth=10, n_estimators=100; total time=   0.5s\n",
      "[CV] END ..learning_rate=0.3, max_depth=10, n_estimators=100; total time=   0.4s\n",
      "[CV] END ..learning_rate=0.3, max_depth=10, n_estimators=200; total time=   0.8s\n",
      "[CV] END ..learning_rate=0.3, max_depth=10, n_estimators=200; total time=   0.8s\n",
      "[CV] END ..learning_rate=0.3, max_depth=10, n_estimators=200; total time=   0.8s\n",
      "[CV] END ..learning_rate=0.3, max_depth=10, n_estimators=200; total time=   0.9s\n",
      "[CV] END ..learning_rate=0.3, max_depth=10, n_estimators=200; total time=   0.9s\n",
      "Best parameters:  {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}\n",
      "테스트 세트 성능:\n",
      "RMSE: 48692.42990052546\n",
      "R2: 0.8269194466626745\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T08:03:47.979282Z",
     "start_time": "2024-09-13T08:03:47.977161Z"
    }
   },
   "cell_type": "code",
   "source": [],
   "id": "851c8de2653d05d2",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 베이지안 최적화로 object function optimize"
   ],
   "id": "db3559f8f510ba22"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T08:03:47.984585Z",
     "start_time": "2024-09-13T08:03:47.980783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from hyperopt import hp\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from hyperopt import STATUS_OK, fmin, tpe, Trials\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_search_space = {\n",
    "    'max_depth': hp.quniform('max_depth', 5, 20, 1),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 2, 1),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "}"
   ],
   "id": "744a97c720de173",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T08:04:29.687529Z",
     "start_time": "2024-09-13T08:03:47.988066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def objective_func(search_space):\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=int(search_space['max_depth']),\n",
    "        min_child_weight=int(search_space['min_child_weight']),\n",
    "        learning_rate=search_space['learning_rate'],\n",
    "        colsample_bytree=search_space['colsample_bytree'],\n",
    "        random_state=123\n",
    "    )\n",
    "    mse = cross_val_score(xgb_reg, train_x, train_y, scoring='neg_mean_squared_error', cv=3)\n",
    "    mse = -1.0 * np.mean(mse)\n",
    "\n",
    "    return {'loss': mse, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "trial_val = Trials()\n",
    "best = fmin(\n",
    "    fn=objective_func,\n",
    "    space=xgb_search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=20,\n",
    "    trials=trial_val,\n",
    "    rstate=np.random.default_rng(seed=9)\n",
    ")\n",
    "\n",
    "print('best:', best)\n"
   ],
   "id": "79dbe95ca3d3b928",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:41<00:00,  2.08s/trial, best loss: 2312353117.2836747]\n",
      "best: {'colsample_bytree': np.float64(0.8637740285716389), 'learning_rate': np.float64(0.10657919742273766), 'max_depth': np.float64(8.0), 'min_child_weight': np.float64(2.0)}\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T08:04:29.691427Z",
     "start_time": "2024-09-13T08:04:29.689411Z"
    }
   },
   "cell_type": "code",
   "source": [],
   "id": "f875d9c2ce04bea8",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### best param\n",
    "\n",
    "colsample_bytree = 0.77084\n",
    "learning_rate = 0.0925\n",
    "max_depth = 9\n",
    "min_child_weight = 2\n"
   ],
   "id": "af094db918e3d40b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T08:04:29.696446Z",
     "start_time": "2024-09-13T08:04:29.692771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "xgb_reg = xgb.XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=8,\n",
    "    min_child_weight=2,\n",
    "    learning_rate=0.09025,\n",
    "    random_state=123\n",
    ")"
   ],
   "id": "9cc2e60d6d658acf",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T08:04:30.212280Z",
     "start_time": "2024-09-13T08:04:29.697927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = xgb_reg.fit(train_x, train_y)"
   ],
   "id": "cf1a0b064576888f",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T08:04:30.226517Z",
     "start_time": "2024-09-13T08:04:30.213815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_predictions = model.predict(test_x)\n",
    "test_mse = mean_squared_error(test_y, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(test_y, test_predictions)\n",
    "\n",
    "print(\"테스트 세트 성능:\")\n",
    "print(\"RMSE:\", test_rmse)\n",
    "print(\"R2:\", test_r2)"
   ],
   "id": "4bfb633fb38ad8b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 세트 성능:\n",
      "RMSE: 48887.31658738394\n",
      "R2: 0.8255311980838452\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T08:04:30.229358Z",
     "start_time": "2024-09-13T08:04:30.227582Z"
    }
   },
   "cell_type": "code",
   "source": [],
   "id": "9b95de5749057cd8",
   "outputs": [],
   "execution_count": 49
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
